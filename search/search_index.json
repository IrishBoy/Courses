{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Courses, Cheatsheet, Learning Plans from digitalbabushka.org \u00b6 Welcome to the Digital Babushka library, here you will find all the knowledge that I've gathered ober the yeats in IT","title":"Courses, Cheatsheet, Learning Plans from digitalbabushka.org"},{"location":"#courses-cheatsheet-learning-plans-from-digitalbabushkaorg","text":"Welcome to the Digital Babushka library, here you will find all the knowledge that I've gathered ober the yeats in IT","title":"Courses, Cheatsheet, Learning Plans from digitalbabushka.org"},{"location":"Courses/Courses/","text":"Available courses \u00b6 Architecture Cheat Sheet","title":"Courses"},{"location":"Courses/Courses/#available-courses","text":"Architecture Cheat Sheet","title":"Available courses"},{"location":"Courses/Application%20Architecture/Architecture%20Cheat%20Sheet%20Introduction/","text":"First of all, this course is not one more \"How to pass System Design interview in MAANG\" or \"Excel your system design interviews\". This course is dedicated towards people who are new to IT or is looking forward broaden there horizons on the field of System Analysis or maybe who thinks that they lack of knowledge in IT. Most of the information in this course was taken from my personal knowledge database (well, for sure before appearing in it, it was either on some site or in one of my projects). So, cut the crap and let start. Number zero, despite the fact that I call it a \"course\", the better name is dictionary or check-list, or whatever. Since I mainly focus not on teaching things, but on giving things that should be learned. \u00b6 Firstly, there are two ways of going through this course Along, going from chapter to chapter as I modelled it Go to the chapters that interest you the most Secondly, Chapter Cases may be presumed as preparation for passing System Design interview, but I would't like to make it the main target of this course. Thirdly, it is not about writing code, it is about structuring logic, modelling architecture, sometimes unorthodox approaches and mostly about interaction between services, not inside service. That means this course is not about writing the code)","title":"Architecture Cheat Sheet Introduction"},{"location":"Courses/Application%20Architecture/Architecture%20Cheat%20Sheet%20Introduction/#number-zero-despite-the-fact-that-i-call-it-a-course-the-better-name-is-dictionary-or-check-list-or-whatever-since-i-mainly-focus-not-on-teaching-things-but-on-giving-things-that-should-be-learned","text":"Firstly, there are two ways of going through this course Along, going from chapter to chapter as I modelled it Go to the chapters that interest you the most Secondly, Chapter Cases may be presumed as preparation for passing System Design interview, but I would't like to make it the main target of this course. Thirdly, it is not about writing code, it is about structuring logic, modelling architecture, sometimes unorthodox approaches and mostly about interaction between services, not inside service. That means this course is not about writing the code)","title":"Number zero, despite the fact that I call it a \"course\", the better name is dictionary or check-list, or whatever. Since I mainly focus not on teaching things, but on giving things that should be learned."},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/1.Basic%20Part/0.%20Request%20Response/","text":"When using any application, you face one of two fundamental approaches Local app Web application The main difference between those types is the need of the internet connection to operate. Local apps, such as Calculator Camera etc. Do not send or receive information from internet in order to work properly When it comes to the apps that require internet, the following ones may be listed Weather forecast News Chess game Those apps use internet. Now, let's pick up the second category and break down the logic of their operating So, there is some logic on your device and some logic on the server But how those two parts communicate? Easy question - over the web If we go deeper, there is a list of protocols that are being used for this communication (Application layer of OSI Model ) HTTP - Hyper Text Transfer Protocol (or more secure HTTPS) FTP - File Transfer Protocol DNS - Domain Name System AMQP - Advanced Message Queuing Protocol SSH - Secure Shell etc. Request Response \u00b6 Request-response model is the main basic pattern when we talk about integrations. The logic behind it is very simple You make a request (call a method) You receive a response Which means that by default no other system can get your response, as well as the fact that initiator and target are known. Of course, we should mention here sync/asycn , but for now it is not that important","title":"0. Request Response"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/1.Basic%20Part/0.%20Request%20Response/#request-response","text":"Request-response model is the main basic pattern when we talk about integrations. The logic behind it is very simple You make a request (call a method) You receive a response Which means that by default no other system can get your response, as well as the fact that initiator and target are known. Of course, we should mention here sync/asycn , but for now it is not that important","title":"Request Response"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/1.Basic%20Part/1.REST/","text":"This is the most popular protocol, we will not spend to much time here This protocol utilises HTTP (S) 2nd or 3rd generation, it consists of the following things Method type - GET in this case GET - get info POST - add new info PUT - full update PATCH - partial update DELETE - delete OPTIONS - get available method types there are some more that are not that widely used URL request - *www.ya.com in this case Subdomain - api in this Endpoint - method in this case Query parameters - type=1 in this case Pagination parameters - page=2&limit=10 in this case Body \u00b6 For every request type (except for GET, OPTIONS and some others) you can have body Body is a payload of the request. What can be body? Well - application/xml - For xml markup - application/json - For JSON - image/png - For png - etc How to manage it ? It is managed in #Headers of the request, in Content-type field Headers \u00b6 Headers are additional data sent with a request to the server, providing key information like the content type of the request body, authentication tokens, or details about the client software. They are distinct from the request body and mainly serve to convey metadata about the request or response. What can store headers? Accept : Indicates the content type that the client expects in the response. Content-Type : Defines the media type of the request body. Authorization : Provides the authentication token or credentials for the request. User-Agent : Identifies the software client making the request. Cache-Control : Controls caching behavior for both the request and response. If-None-Match : Makes a conditional request based on the ETag header value.","title":"1.REST"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/1.Basic%20Part/1.REST/#body","text":"For every request type (except for GET, OPTIONS and some others) you can have body Body is a payload of the request. What can be body? Well - application/xml - For xml markup - application/json - For JSON - image/png - For png - etc How to manage it ? It is managed in #Headers of the request, in Content-type field","title":"Body"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/1.Basic%20Part/1.REST/#headers","text":"Headers are additional data sent with a request to the server, providing key information like the content type of the request body, authentication tokens, or details about the client software. They are distinct from the request body and mainly serve to convey metadata about the request or response. What can store headers? Accept : Indicates the content type that the client expects in the response. Content-Type : Defines the media type of the request body. Authorization : Provides the authentication token or credentials for the request. User-Agent : Identifies the software client making the request. Cache-Control : Controls caching behavior for both the request and response. If-None-Match : Makes a conditional request based on the ETag header value.","title":"Headers"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/1.Basic%20Part/2.gRPC/","text":"gRPC - Google Remote Procedure Call Works over HTTP , and gives more flexibility than REST , however there are not many libraries for front-end languages. And it uses protobuf protocol Each request consists of the following parts Request name Request body Response body syntax = \"proto3\"; import \"google/protobuf/timestamp.proto\"; service BillingService{ rpc GetBalance(GetBalanceRequest) returns (GetBalanceResponse); } message GetBalanceRequest{ repeated string account_ids = 1; } message GetBalanceResponse{ repeated Account accounts = 1; } message Account{ string user_id = 1; string account_id = 2; AccountType account_type = 3; int64 balance = 4; string currecny = 5; google.protobuf.Timestamp updated_at = 6; } enum AccountType{ ACCOUNT_TYPE_UNDIFINED = 0; ACCOUNT_TYPE_1 = 1; ACCOUNT_TYPE_2 = 2; ACCOUNT_TYPE_3 = 3; } service BillingService - could be described as package or group of methods GetBalance - method name GetBalanceRequest - Request, could be empty GetBalanceResponse - Response, also could be empty Each message (response or request) consists of several (may be no one) fields Each field can be set //String value string user_id = 1 ; //Array of int8 repeated int8 ids = 2 ; //Custom message type CustomMessage custom_message = 3 ; Why to use grcp instead of REST? \u00b6 No artificial restrictions in terms of method types Besides the fact that we can use GET, POST etc. as we wish, it still may bring some frustration More flexible types There are more variety of types, for example int4, timestamp etc. Speed You can see on the graph that gRPC is much faster on big data Here is the link","title":"2.gRPC"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/1.Basic%20Part/2.gRPC/#why-to-use-grcp-instead-of-rest","text":"No artificial restrictions in terms of method types Besides the fact that we can use GET, POST etc. as we wish, it still may bring some frustration More flexible types There are more variety of types, for example int4, timestamp etc. Speed You can see on the graph that gRPC is much faster on big data Here is the link","title":"Why to use grcp instead of REST?"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/1.Basic%20Part/3.graphQL/","text":"This API protocol also works over HTTP , however, it gives more flexibility in terms of requests. graphQL focuses on getting specific information that is needed, without any extra. The principle is based on making sql-like requests retrieving exact information you need For example, let's imagine that we have a service that is responsible for movies, directors etc. Case 1: You need to get director's birthdate, but in the director model there are 30 additional fields that you don't really need Case 2: You need to get name of directors fathers dog sister, with other API types you would need to make many requests, with graphQL you can extract required information with one request. Basic request example { # Object name hero { # Object parametrs name } } Response { \"data\": { \"hero\": { \"name\": \"R2-D2\" } } } It is also possible to filter Request { human(id: \"1000\") { name height } } Response { \"data\": { \"human\": { \"name\": \"Luke Skywalker\", \"height\": 1.72 } } } etc. More detailed information may be find here https://graphql.org/learn/queries/","title":"3.graphQL"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/1.Basic%20Part/4.Pub-sub%20vs%20message%20queue/","text":"Queues and pub/sub are another way of communication between applications. Imagine some sort of jar, someone puts there a paper with text, then another person comes and takes it. So, this is the main idea of queues, publisher puts some message into the queue, then subscriber reads it. Without any doubt we can configure the whole process according to the following parameters How many times message could be read Who can publish messages Whether the messages should be logged etc But for now it is not that important so, what's the difference? Feature Message Queue (MQ) Publisher-Subscriber (Pub-Sub) Concept of Message Messages generally have a specific destination. Messages, also known as events , are sent to multiple subscribers. Message Delivery Messages are delivered to one consumer. Messages are delivered to all subscribers. Event Retry Support exists, with the use of a dead letter queue . Depends on the queue implemented under the hood. Memory Requires significant amount of memory. Requires less memory as messages are discarded post distribution. Reliability Higher, thanks to the acknowledgement concept. Lower, due to lack of acknowledgment. Throughput Lower, due to the processing of a single message at a time. Higher, as multiple messages can be processed parallelly. Common Usage Task distribution, load balancing. Multi-casting events, data streaming.","title":"4.Pub sub vs message queue"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/1.Basic%20Part/5.kafka/","text":"Kafka is the most popular message queue in today's world It works over TCP Here you can see how it is structured So, what are partitions, topics, producer, consumer and consumer groups ? Producer - services that publishes messages to the broker Topic - category, or folder, or any analogy you can come up with, it is more logical unit that helps us to separate messages Partitions - every topic may be distributed across different nodes, those distributed units are called partitions Consumer and consumer group - those are services that are going to consume messages, the main difference is If two consumers have subscribed to the same topic and are present in the same consumer group, then these two consumers would be assigned a different set of partitions and none of these two consumers would receive the same messages.","title":"5.kafka"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/2.Average%20Part/1.Web%20Sockets/","text":"So, what if you want to have bidirectional connection since you need lower latency ? Well, then you need web sockets Within this approach TCP is used and connection is up until client or server shuts it down. Step 1. The client initiates an HTTP connection. Step 2. WebSocket establishes a handshake between the client and server that makes WebSocket compatible with HTTP ports (80 and 443) and proxies. Step 3. Both the client and server exchange messages freely via an open, persistent connection that doesn\u2019t require continuous polling or new HTTP requests. Step 4. The connection remains open until one side closes the channel, at which point the connection closes and the client-server engagement ends. It is easy to understand pros but what are cons? Increased Complexity Resource Consumption Firewall and Proxy Issues Security Concerns Lack of Built-in Retry Mechanism Incompatible with REST APIs Monitoring and Debugging complexity No Native Support for Caching","title":"1.Web Sockets"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/2.Average%20Part/2.Web%20hooks%20and%20Polling/","text":"Imagine a classic family road trip: Polling is like the kids constantly asking, \"Are we there yet?\" and the parents repeatedly responding, \"No.\" Webhooks are like the kids peacefully sleeping in the backseat, only being woken up by the parents when they've reached the destination. With webhooks, there's no need to keep checking for updates. Instead, the service provider sends the data to a specified URL as soon as it's available.","title":"2.Web hooks and Polling"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/2.Average%20Part/3.RabbitMQ/","text":"RabbitMQ \u00b6 RabbitMQ is another example of message broker It works on AMQP Here you can observe architecture So, the main difference from kafka is the presence of Exchange : Receives messages from producers and routes them to queues based on the rules set by the exchange type. For a queue to receive messages, it must be bound to at least one exchange. Direct Exchange : Routes messages to queues based on an exact match between the message routing key and the queue's binding key. Fanout Exchange : Broadcasts messages to all queues bound to the exchange, ignoring the routing key. Topic Exchange : Routes messages to queues based on pattern matching between the routing key and the queue's binding key, allowing flexible and wildcard matching. Headers Exchange : Routes messages based on matching message header values rather than the routing key. Binding : A binding creates a connection between a queue and an exchange.","title":"3.RabbitMQ"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/2.Average%20Part/3.RabbitMQ/#rabbitmq","text":"RabbitMQ is another example of message broker It works on AMQP Here you can observe architecture So, the main difference from kafka is the presence of Exchange : Receives messages from producers and routes them to queues based on the rules set by the exchange type. For a queue to receive messages, it must be bound to at least one exchange. Direct Exchange : Routes messages to queues based on an exact match between the message routing key and the queue's binding key. Fanout Exchange : Broadcasts messages to all queues bound to the exchange, ignoring the routing key. Topic Exchange : Routes messages to queues based on pattern matching between the routing key and the queue's binding key, allowing flexible and wildcard matching. Headers Exchange : Routes messages based on matching message header values rather than the routing key. Binding : A binding creates a connection between a queue and an exchange.","title":"RabbitMQ"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/2.Average%20Part/4.%20SDK/","text":"SDK - software development kit This is a tool that allows you to communicate with some external system using your programming language. Sounds a bit frustrating, so, let's say that SDK == a couple of libraries Here you can see difference between API, Library and SDK What about examples ? \u00b6 Asana SDK \u00b6 // Install by running `npm i asana` const Asana = require ( 'asana' ); let asanaClient = Asana . ApiClient . instance ; let clientToken = asanaClient . authentications [ 'token' ]; clientToken . accessToken = '' ; let tasksApiInstance = new Asana . TasksApi (); let options = { \"limit\" : 50 , \"project\" : \"\" , }; const response = await tasksApiInstance . getTasks ( options ) console . log ( response ) This code allows to create a task in Asana without making any http request manualy","title":"4. SDK"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/2.Average%20Part/4.%20SDK/#what-about-examples","text":"","title":"What about examples ?"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/2.Average%20Part/4.%20SDK/#asana-sdk","text":"// Install by running `npm i asana` const Asana = require ( 'asana' ); let asanaClient = Asana . ApiClient . instance ; let clientToken = asanaClient . authentications [ 'token' ]; clientToken . accessToken = '' ; let tasksApiInstance = new Asana . TasksApi (); let options = { \"limit\" : 50 , \"project\" : \"\" , }; const response = await tasksApiInstance . getTasks ( options ) console . log ( response ) This code allows to create a task in Asana without making any http request manualy","title":"Asana SDK"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/2.Average%20Part/5.%20ESB/","text":"Enterprise Service Bus - a huge thing for huge systems Basically it is the mechanism to keep all the components on the same side regarding the data Meaning that this is a mechanism to support real live data exchange between all the applications inside the system It is vital to say, that it is not a technology it is a pattern , this means that ESB can be implemented via endless array of different technologies. So, let's draw (obviously I've stolen this picture from some article) Here it is shown that different systems use ESB in order to get and send data across all other services. It is vital to state that this approach should only be used when the system may be called big and you have an enormous amount of data How you can implement it ? \u00b6 Message broker that supports fan out principles A set of services that pulls and updates the data from each service to each service etc What technologies to look at ? \u00b6 Apache Camel WSO2 Enterprise Integrator MuleSoft Anypoint Platform Talend ESB","title":"5. ESB"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/2.Average%20Part/5.%20ESB/#how-you-can-implement-it","text":"Message broker that supports fan out principles A set of services that pulls and updates the data from each service to each service etc","title":"How you can implement it ?"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/2.Average%20Part/5.%20ESB/#what-technologies-to-look-at","text":"Apache Camel WSO2 Enterprise Integrator MuleSoft Anypoint Platform Talend ESB","title":"What technologies to look at ?"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/1.RedPanda/","text":"Redpanda is a source available (BSL), Apache Kafka\u00ae-compatible, streaming data platform designed from the ground up to be lighter, faster, and simpler to operate. It employs a single binary architecture, free from ZooKeeper and JVMs, with a built-in Schema Registry and HTTP Proxy. No Java : A JVM-free and ZooKeeper-free infrastructure. Designed in C++ : Designed for a better performance than Apache Kafka. A single-binary architecture : No dependencies to other libraries or nodes. Self-managing and self-healing : A simple but scalable architecture for on-premise and cloud deployments. Kafka-compatible : Out-of-the-box support for the Kafka protocol with existing applications, tools, and integrations. Kafka vs Red Panda \u00b6 Kafka Red panda License Open source Under the Apache License governed by the Apache Software Foundation. Source available Under the Business Source License (BSL) with proprietary paid features available under an enterprise license. Contribution model and commercial backing Open Actively managed and maintained by 1,000+ full-time contributors at over a dozen companies and commercially backed by a broad coalition of vendors. Restricted Solely developed and maintained by Redpanda, with restrictive commercial support from other vendors due to BSL license agreement. Source Language Java C++ ZooKeeper Dependency No dependency ZooKeeper was removed by KRaft since version 3.3+ No dependency ZooKeeper-free and uses the Raft consensus algorithm. Storage Pattern and Performance Impact Consistent performance across most real-world workloads Kafka has a purpose-built log and replication layer optimized for sequential IO, which allows it to deliver high throughput and low latency across a broad set of hardware and workloads. Performance optimized for selective workloads Redpanda can demonstrate low latency and high throughput on simple workloads. However, because it\u2019s optimized for random IO, its performance can significantly degrade over time. Several common production configurations, such as high producer count, over 30% disk utilization, enabling message keys, enabling TLS, or running for more than 24 hours can cause severe reductions in performance. Broker Framework Purpose-built immutable log Uses its own purpose-built framework. Data is written in large blocks as high throughput sequential IO, allowing for high performance on drives with even very low IOPS. Based on Seastar Uses the Seastar framework, popularized by the Scylla Database, to implement its immutable log. Writes data in small 16kB chunks by default, requiring very high IOPS SSDs. Tiered Storage In Progress with KIP-405 Slated for early access in Kafka release 3.6. Requires Enterprise License Redpanda\u2019s tiered storage requires the purchase of an enterprise license. Replication Protocol Kafka replication (ISR) Replication is synchronous but data is written to disk asynchronously by design. Brokers don\u2019t need to fsync for correctness and have in-built data recovery and repair. Raft protocol Both replication and writing to disk are synchronous. Data must be written (fsynced) to disk synchronously, otherwise, it is possible to lose data during an election of a new leader. Cloud Network Optimized Optimized for cloud Follower Fetching enables clients to read data from follower replicas in the current AZ, avoiding cross-AZ network costs Cloud optimization in beta Follower fetching recently released in version 23.2. Connectors and Stream Processing Included Kafka Connect and Kafka Streams are packaged as part of the core open source Kafka offering. These two components allow you to connect applications and databases together and process data streams at scale. Not Included Not included with Redpanda. While Kafka Connect and Kafka Streams are compatible, they require you to configure, manage, and scale your own JVM-based applications and jobs. Breadth of adoption Vast developer ecosystem and community Apache Kafka is used by 100,000+ organizations, including 80% of F100 companies, including Goldman Sachs, Netflix and Uber Limited adoption and community Redpanda is used by thousands of organizations (undisclosed)","title":"1.RedPanda"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/1.RedPanda/#kafka-vs-red-panda","text":"Kafka Red panda License Open source Under the Apache License governed by the Apache Software Foundation. Source available Under the Business Source License (BSL) with proprietary paid features available under an enterprise license. Contribution model and commercial backing Open Actively managed and maintained by 1,000+ full-time contributors at over a dozen companies and commercially backed by a broad coalition of vendors. Restricted Solely developed and maintained by Redpanda, with restrictive commercial support from other vendors due to BSL license agreement. Source Language Java C++ ZooKeeper Dependency No dependency ZooKeeper was removed by KRaft since version 3.3+ No dependency ZooKeeper-free and uses the Raft consensus algorithm. Storage Pattern and Performance Impact Consistent performance across most real-world workloads Kafka has a purpose-built log and replication layer optimized for sequential IO, which allows it to deliver high throughput and low latency across a broad set of hardware and workloads. Performance optimized for selective workloads Redpanda can demonstrate low latency and high throughput on simple workloads. However, because it\u2019s optimized for random IO, its performance can significantly degrade over time. Several common production configurations, such as high producer count, over 30% disk utilization, enabling message keys, enabling TLS, or running for more than 24 hours can cause severe reductions in performance. Broker Framework Purpose-built immutable log Uses its own purpose-built framework. Data is written in large blocks as high throughput sequential IO, allowing for high performance on drives with even very low IOPS. Based on Seastar Uses the Seastar framework, popularized by the Scylla Database, to implement its immutable log. Writes data in small 16kB chunks by default, requiring very high IOPS SSDs. Tiered Storage In Progress with KIP-405 Slated for early access in Kafka release 3.6. Requires Enterprise License Redpanda\u2019s tiered storage requires the purchase of an enterprise license. Replication Protocol Kafka replication (ISR) Replication is synchronous but data is written to disk asynchronously by design. Brokers don\u2019t need to fsync for correctness and have in-built data recovery and repair. Raft protocol Both replication and writing to disk are synchronous. Data must be written (fsynced) to disk synchronously, otherwise, it is possible to lose data during an election of a new leader. Cloud Network Optimized Optimized for cloud Follower Fetching enables clients to read data from follower replicas in the current AZ, avoiding cross-AZ network costs Cloud optimization in beta Follower fetching recently released in version 23.2. Connectors and Stream Processing Included Kafka Connect and Kafka Streams are packaged as part of the core open source Kafka offering. These two components allow you to connect applications and databases together and process data streams at scale. Not Included Not included with Redpanda. While Kafka Connect and Kafka Streams are compatible, they require you to configure, manage, and scale your own JVM-based applications and jobs. Breadth of adoption Vast developer ecosystem and community Apache Kafka is used by 100,000+ organizations, including 80% of F100 companies, including Goldman Sachs, Netflix and Uber Limited adoption and community Redpanda is used by thousands of organizations (undisclosed)","title":"Kafka vs Red Panda"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/2.Short%20polling%20VS%20%20Long%20polling/","text":"Short Polling \u00b6 What is Short Polling? Short polling works like periodic check-ins between a web browser and a server. It's similar to texting a friend and repeatedly asking, \"Any updates?\" When to Use Short Polling? Short polling is best for situations where immediate updates aren't essential. For example, apps that refresh data periodically, like weather apps, can benefit from short polling. Advantages: Simplicity: Short polling is easy to implement, making it suitable for a variety of applications. Compatibility: It functions well with most server environments and doesn\u2019t need complex configurations. Disadvantages: Excessive Requests: Regular, frequent requests can cause unnecessary server load and increase network traffic. Latency: Since updates are only fetched during polling intervals, it can lead to delays in receiving new information. Long polling: \u00b6 What is Long Polling? Long polling is like keeping an open conversation between the browser and server, where the server waits to respond until there's new information to share, reducing the need for constant check-ins. When to Use Long Polling? Long polling is ideal for scenarios where real-time updates are important, but you want to avoid excessive requests. It works well for applications like messaging apps, where you need immediate updates without repeatedly asking, \"Any new messages?\" Advantages: Reduced Requests: The server keeps the connection open until new data is available, cutting down on unnecessary requests. Near Real-Time Updates: Long polling delivers updates faster than short polling, offering a more responsive user experience. Disadvantages: Resource Intensive: Holding long-lived connections can put a strain on server resources, particularly if the system isn't optimized for it. Connection Loss Latency: If the connection drops, there can be delays in re-establishing it, affecting the user experience.","title":"2.Short polling VS  Long polling"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/2.Short%20polling%20VS%20%20Long%20polling/#short-polling","text":"What is Short Polling? Short polling works like periodic check-ins between a web browser and a server. It's similar to texting a friend and repeatedly asking, \"Any updates?\" When to Use Short Polling? Short polling is best for situations where immediate updates aren't essential. For example, apps that refresh data periodically, like weather apps, can benefit from short polling. Advantages: Simplicity: Short polling is easy to implement, making it suitable for a variety of applications. Compatibility: It functions well with most server environments and doesn\u2019t need complex configurations. Disadvantages: Excessive Requests: Regular, frequent requests can cause unnecessary server load and increase network traffic. Latency: Since updates are only fetched during polling intervals, it can lead to delays in receiving new information.","title":"Short Polling"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/2.Short%20polling%20VS%20%20Long%20polling/#long-polling","text":"What is Long Polling? Long polling is like keeping an open conversation between the browser and server, where the server waits to respond until there's new information to share, reducing the need for constant check-ins. When to Use Long Polling? Long polling is ideal for scenarios where real-time updates are important, but you want to avoid excessive requests. It works well for applications like messaging apps, where you need immediate updates without repeatedly asking, \"Any new messages?\" Advantages: Reduced Requests: The server keeps the connection open until new data is available, cutting down on unnecessary requests. Near Real-Time Updates: Long polling delivers updates faster than short polling, offering a more responsive user experience. Disadvantages: Resource Intensive: Holding long-lived connections can put a strain on server resources, particularly if the system isn't optimized for it. Connection Loss Latency: If the connection drops, there can be delays in re-establishing it, affecting the user experience.","title":"Long polling:"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/3.Apache%20Thrift/","text":"Apache Thrift is something like gRPC BUT it can use binary protocol/ JSON protocol, and more It has another IDL (Interface Definition Language) It doesn't use HTTP /2 as its transport protocol, but it can be adapted to use it How does it look like ? \u00b6 Well, first of all here is Thrift Architecture for some nerds In order to start working you need to define interfaces data structures Whole process may be find here // ProductService.thrift namespace java com.example namespace py example struct Product { 1: i32 id, 2: string name, 3: double price, // ... other fields ... } service ProductService { Product getProductByID(1: i32 productID), list<Product> searchProducts(1: string query), bool updateProductStock(1: i32 productID, 2: i32 quantity), // ... other methods ... } After that you can generate Server (example in Java) try { TServerTransport serverTransport = new TServerSocket ( 9090 ); TServer server = new TSimpleServer ( new Args ( serverTransport ). processor ( processor )); // Use this for a multithreaded server // TServer server = new TThreadPoolServer(new TThreadPoolServer.Args(serverTransport).processor(processor)); System . out . println ( \"Starting the simple server...\" ); server . serve (); } catch ( Exception e ) { e . printStackTrace (); } And also handlers public class CalculatorHandler implements Calculator . Iface { private HashMap < Integer , SharedStruct > log ; public CalculatorHandler () { log = new HashMap < Integer , SharedStruct > (); } public void ping () { System . out . println ( \"ping()\" ); } public int add ( int n1 , int n2 ) { System . out . println ( \"add(\" + n1 + \",\" + n2 + \")\" ); return n1 + n2 ; } public int calculate ( int logid , Work work ) throws InvalidOperation { System . out . println ( \"calculate(\" + logid + \", {\" + work . op + \",\" + work . num1 + \",\" + work . num2 + \"})\" ); int val = 0 ; switch ( work . op ) { case ADD : val = work . num1 + work . num2 ; break ; case SUBTRACT : val = work . num1 - work . num2 ; break ; case MULTIPLY : val = work . num1 * work . num2 ; break ; case DIVIDE : if ( work . num2 == 0 ) { InvalidOperation io = new InvalidOperation (); io . whatOp = work . op . getValue (); io . why = \"Cannot divide by 0\" ; throw io ; } val = work . num1 / work . num2 ; break ; default : InvalidOperation io = new InvalidOperation (); io . whatOp = work . op . getValue (); io . why = \"Unknown operation\" ; throw io ; } SharedStruct entry = new SharedStruct (); entry . key = logid ; entry . value = Integer . toString ( val ); log . put ( logid , entry ); return val ; } public SharedStruct getStruct ( int key ) { System . out . println ( \"getStruct(\" + key + \")\" ); return log . get ( key ); } public void zip () { System . out . println ( \"zip()\" ); } }","title":"3.Apache Thrift"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/3.Apache%20Thrift/#how-does-it-look-like","text":"Well, first of all here is Thrift Architecture for some nerds In order to start working you need to define interfaces data structures Whole process may be find here // ProductService.thrift namespace java com.example namespace py example struct Product { 1: i32 id, 2: string name, 3: double price, // ... other fields ... } service ProductService { Product getProductByID(1: i32 productID), list<Product> searchProducts(1: string query), bool updateProductStock(1: i32 productID, 2: i32 quantity), // ... other methods ... } After that you can generate Server (example in Java) try { TServerTransport serverTransport = new TServerSocket ( 9090 ); TServer server = new TSimpleServer ( new Args ( serverTransport ). processor ( processor )); // Use this for a multithreaded server // TServer server = new TThreadPoolServer(new TThreadPoolServer.Args(serverTransport).processor(processor)); System . out . println ( \"Starting the simple server...\" ); server . serve (); } catch ( Exception e ) { e . printStackTrace (); } And also handlers public class CalculatorHandler implements Calculator . Iface { private HashMap < Integer , SharedStruct > log ; public CalculatorHandler () { log = new HashMap < Integer , SharedStruct > (); } public void ping () { System . out . println ( \"ping()\" ); } public int add ( int n1 , int n2 ) { System . out . println ( \"add(\" + n1 + \",\" + n2 + \")\" ); return n1 + n2 ; } public int calculate ( int logid , Work work ) throws InvalidOperation { System . out . println ( \"calculate(\" + logid + \", {\" + work . op + \",\" + work . num1 + \",\" + work . num2 + \"})\" ); int val = 0 ; switch ( work . op ) { case ADD : val = work . num1 + work . num2 ; break ; case SUBTRACT : val = work . num1 - work . num2 ; break ; case MULTIPLY : val = work . num1 * work . num2 ; break ; case DIVIDE : if ( work . num2 == 0 ) { InvalidOperation io = new InvalidOperation (); io . whatOp = work . op . getValue (); io . why = \"Cannot divide by 0\" ; throw io ; } val = work . num1 / work . num2 ; break ; default : InvalidOperation io = new InvalidOperation (); io . whatOp = work . op . getValue (); io . why = \"Unknown operation\" ; throw io ; } SharedStruct entry = new SharedStruct (); entry . key = logid ; entry . value = Integer . toString ( val ); log . put ( logid , entry ); return val ; } public SharedStruct getStruct ( int key ) { System . out . println ( \"getStruct(\" + key + \")\" ); return log . get ( key ); } public void zip () { System . out . println ( \"zip()\" ); } }","title":"How does it look like ?"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/4.Avro/","text":"Apache Avro is a data serialization framework Basically, it is another way of serializing JSON Key features of Avro include: Compact Serialization : Avro uses a compact binary format f Schema-Based : Avro relies on schemas to define the structure of the data being serialized. The schema is stored alongside the data, allowing for schema evolution. This means you can change the schema over time (e.g., adding or removing fields) without breaking backward or forward compatibility. Interoperability : Since the schema is embedded with the data, any system that reads Avro-serialized data can interpret it without needing additional information, making it great for distributed systems. Typical Use Cases: \u00b6 Data Storage : Avro is often used with systems like Hadoop, Apache Kafka, and Apache Flink for storing large amounts of data efficiently. Data Interchange : Since Avro is language-neutral, it is widely used for cross-language RPC (Remote Procedure Calls) and message passing in distributed systems. It is commonly paired with other technologies like 5.kafka for message queuing, or Hadoop for big data storage and processing. How does it work? \u00b6 1. Basic Avro Schema and Serialization (Python) \u00b6 First, you define the Avro schema in JSON format. This schema will describe the structure of the data you're going to serialize. Avro Schema (JSON) \u00b6 { \"type\" : \"record\" , \"name\" : \"User\" , \"fields\" : [ { \"name\" : \"name\" , \"type\" : \"string\" }, { \"name\" : \"age\" , \"type\" : \"int\" }, { \"name\" : \"email\" , \"type\" : [ \"null\" , \"string\" ], \"default\" : null } ] } This schema describes a User record with three fields: name (string), age (integer), and email (nullable string with a default value of null ). Python Example (Serialization and Deserialization) \u00b6 Now, let's use Python to serialize and deserialize data based on this Avro schema. Install the avro package if you haven't already: pip install avro-python3 Example Python script: import avro.schema import avro.io import io schema_json = \"\"\" { \"type\": \"record\", \"name\": \"User\", \"fields\": [ {\"name\": \"name\", \"type\": \"string\"}, {\"name\": \"age\", \"type\": \"int\"}, {\"name\": \"email\", \"type\": [\"null\", \"string\"], \"default\": null} ] } \"\"\" schema = avro . schema . parse ( schema_json ) # Create an example user record user = { \"name\" : \"John Doe\" , \"age\" : 28 , \"email\" : \"john.doe@example.com\" } bytes_writer = io . BytesIO () encoder = avro . io . BinaryEncoder ( bytes_writer ) datum_writer = avro . io . DatumWriter ( schema ) datum_writer . write ( user , encoder ) serialized_data = bytes_writer . getvalue () print ( f \"Serialized data: { serialized_data } \" ) bytes_reader = io . BytesIO ( serialized_data ) decoder = avro . io . BinaryDecoder ( bytes_reader ) datum_reader = avro . io . DatumReader ( schema ) deserialized_user = datum_reader . read ( decoder ) print ( f \"Deserialized data: { deserialized_user } \" ) Output: \u00b6 Serialized data: b'\\x06John Doe8\\x1ejohn.doe@example.com' Deserialized data: {'name': 'John Doe', 'age': 28, 'email': 'john.doe@example.com'} 2. Avro with Nullable Fields \u00b6 Avro supports nullable fields using a union type . Here's an example where the email field can either be a string or null . Schema with Nullable Field (JSON) \u00b6 { \"type\" : \"record\" , \"name\" : \"User\" , \"fields\" : [ { \"name\" : \"name\" , \"type\" : \"string\" }, { \"name\" : \"age\" , \"type\" : \"int\" }, { \"name\" : \"email\" , \"type\" : [ \"null\" , \"string\" ], \"default\" : null } ] } Python Example (Handling Null Values) \u00b6 user_without_email = { \"name\" : \"Jane Doe\" , \"age\" : 25 , \"email\" : None } bytes_writer = io . BytesIO () encoder = avro . io . BinaryEncoder ( bytes_writer ) datum_writer = avro . io . DatumWriter ( schema ) datum_writer . write ( user_without_email , encoder ) serialized_data = bytes_writer . getvalue () print ( f \"Serialized data (without email): { serialized_data } \" ) bytes_reader = io . BytesIO ( serialized_data ) decoder = avro . io . BinaryDecoder ( bytes_reader ) deserialized_user = datum_reader . read ( decoder ) print ( f \"Deserialized data (without email): { deserialized_user } \" ) Output: \u00b6 Serialized data (without email): b'\\x0cJane Doe2\\x00' Deserialized data (without email): {'name': 'Jane Doe', 'age': 25, 'email': None} 3. Writing Avro Data to a File (Python) \u00b6 In real-world applications, Avro data is often written to files, especially in big data processing. Here's an example of how to write serialized Avro data to a file. Writing Avro Data to a File: \u00b6 import avro.datafile import avro.io import avro.schema import io schema_json = \"\"\" { \"type\": \"record\", \"name\": \"User\", \"fields\": [ {\"name\": \"name\", \"type\": \"string\"}, {\"name\": \"age\", \"type\": \"int\"}, {\"name\": \"email\", \"type\": [\"null\", \"string\"], \"default\": null} ] } \"\"\" schema = avro . schema . parse ( schema_json ) users = [ { \"name\" : \"John Doe\" , \"age\" : 28 , \"email\" : \"john.doe@example.com\" }, { \"name\" : \"Jane Doe\" , \"age\" : 25 , \"email\" : None } ] with open ( \"users.avro\" , \"wb\" ) as avro_file : writer = avro . datafile . DataFileWriter ( avro_file , avro . io . DatumWriter (), schema ) for user in users : writer . append ( user ) writer . close () print ( \"Data written to users.avro\" ) Reading Avro Data from a File: \u00b6 with open ( \"users.avro\" , \"rb\" ) as avro_file : reader = avro . datafile . DataFileReader ( avro_file , avro . io . DatumReader ()) for user in reader : print ( user ) reader . close () Output: \u00b6 {'name': 'John Doe', 'age': 28, 'email': 'john.doe@example.com'} {'name': 'Jane Doe', 'age': 25, 'email': None} 4. Schema Evolution Example \u00b6 One of Avro\u2019s powerful features is schema evolution . Let\u2019s say you add a new field to the schema but still want to read data serialized with the old schema. Original Schema (Old): \u00b6 { \"type\" : \"record\" , \"name\" : \"User\" , \"fields\" : [ { \"name\" : \"name\" , \"type\" : \"string\" }, { \"name\" : \"age\" , \"type\" : \"int\" } ] } New Schema (Evolved): \u00b6 { \"type\" : \"record\" , \"name\" : \"User\" , \"fields\" : [ { \"name\" : \"name\" , \"type\" : \"string\" }, { \"name\" : \"age\" , \"type\" : \"int\" }, { \"name\" : \"email\" , \"type\" : [ \"null\" , \"string\" ], \"default\" : null } ] } Python Example (Handling Schema Evolution): \u00b6 If data was written with the old schema, you can still read it with the new schema that has the email field added. new_schema_json = \"\"\" { \"type\": \"record\", \"name\": \"User\", \"fields\": [ {\"name\": \"name\", \"type\": \"string\"}, {\"name\": \"age\", \"type\": \"int\"}, {\"name\": \"email\", \"type\": [\"null\", \"string\"], \"default\": null} ] } \"\"\" new_schema = avro . schema . parse ( new_schema_json ) with open ( \"users_old.avro\" , \"rb\" ) as avro_file : reader = avro . datafile . DataFileReader ( avro_file , avro . io . DatumReader ( new_schema )) for user in reader : print ( user ) reader . close () Output: \u00b6 ```plaintext {'name': 'John Doe', 'age': 28, 'email': None} {'name': 'Jane Doe', 'age': 25, 'email': None}","title":"4.Avro"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/4.Avro/#typical-use-cases","text":"Data Storage : Avro is often used with systems like Hadoop, Apache Kafka, and Apache Flink for storing large amounts of data efficiently. Data Interchange : Since Avro is language-neutral, it is widely used for cross-language RPC (Remote Procedure Calls) and message passing in distributed systems. It is commonly paired with other technologies like 5.kafka for message queuing, or Hadoop for big data storage and processing.","title":"Typical Use Cases:"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/4.Avro/#how-does-it-work","text":"","title":"How does it work?"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/4.Avro/#1-basic-avro-schema-and-serialization-python","text":"First, you define the Avro schema in JSON format. This schema will describe the structure of the data you're going to serialize.","title":"1. Basic Avro Schema and Serialization (Python)"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/4.Avro/#avro-schema-json","text":"{ \"type\" : \"record\" , \"name\" : \"User\" , \"fields\" : [ { \"name\" : \"name\" , \"type\" : \"string\" }, { \"name\" : \"age\" , \"type\" : \"int\" }, { \"name\" : \"email\" , \"type\" : [ \"null\" , \"string\" ], \"default\" : null } ] } This schema describes a User record with three fields: name (string), age (integer), and email (nullable string with a default value of null ).","title":"Avro Schema (JSON)"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/4.Avro/#python-example-serialization-and-deserialization","text":"Now, let's use Python to serialize and deserialize data based on this Avro schema. Install the avro package if you haven't already: pip install avro-python3 Example Python script: import avro.schema import avro.io import io schema_json = \"\"\" { \"type\": \"record\", \"name\": \"User\", \"fields\": [ {\"name\": \"name\", \"type\": \"string\"}, {\"name\": \"age\", \"type\": \"int\"}, {\"name\": \"email\", \"type\": [\"null\", \"string\"], \"default\": null} ] } \"\"\" schema = avro . schema . parse ( schema_json ) # Create an example user record user = { \"name\" : \"John Doe\" , \"age\" : 28 , \"email\" : \"john.doe@example.com\" } bytes_writer = io . BytesIO () encoder = avro . io . BinaryEncoder ( bytes_writer ) datum_writer = avro . io . DatumWriter ( schema ) datum_writer . write ( user , encoder ) serialized_data = bytes_writer . getvalue () print ( f \"Serialized data: { serialized_data } \" ) bytes_reader = io . BytesIO ( serialized_data ) decoder = avro . io . BinaryDecoder ( bytes_reader ) datum_reader = avro . io . DatumReader ( schema ) deserialized_user = datum_reader . read ( decoder ) print ( f \"Deserialized data: { deserialized_user } \" )","title":"Python Example (Serialization and Deserialization)"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/4.Avro/#output","text":"Serialized data: b'\\x06John Doe8\\x1ejohn.doe@example.com' Deserialized data: {'name': 'John Doe', 'age': 28, 'email': 'john.doe@example.com'}","title":"Output:"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/4.Avro/#2-avro-with-nullable-fields","text":"Avro supports nullable fields using a union type . Here's an example where the email field can either be a string or null .","title":"2. Avro with Nullable Fields"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/4.Avro/#schema-with-nullable-field-json","text":"{ \"type\" : \"record\" , \"name\" : \"User\" , \"fields\" : [ { \"name\" : \"name\" , \"type\" : \"string\" }, { \"name\" : \"age\" , \"type\" : \"int\" }, { \"name\" : \"email\" , \"type\" : [ \"null\" , \"string\" ], \"default\" : null } ] }","title":"Schema with Nullable Field (JSON)"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/4.Avro/#python-example-handling-null-values","text":"user_without_email = { \"name\" : \"Jane Doe\" , \"age\" : 25 , \"email\" : None } bytes_writer = io . BytesIO () encoder = avro . io . BinaryEncoder ( bytes_writer ) datum_writer = avro . io . DatumWriter ( schema ) datum_writer . write ( user_without_email , encoder ) serialized_data = bytes_writer . getvalue () print ( f \"Serialized data (without email): { serialized_data } \" ) bytes_reader = io . BytesIO ( serialized_data ) decoder = avro . io . BinaryDecoder ( bytes_reader ) deserialized_user = datum_reader . read ( decoder ) print ( f \"Deserialized data (without email): { deserialized_user } \" )","title":"Python Example (Handling Null Values)"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/4.Avro/#output_1","text":"Serialized data (without email): b'\\x0cJane Doe2\\x00' Deserialized data (without email): {'name': 'Jane Doe', 'age': 25, 'email': None}","title":"Output:"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/4.Avro/#3-writing-avro-data-to-a-file-python","text":"In real-world applications, Avro data is often written to files, especially in big data processing. Here's an example of how to write serialized Avro data to a file.","title":"3. Writing Avro Data to a File (Python)"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/4.Avro/#writing-avro-data-to-a-file","text":"import avro.datafile import avro.io import avro.schema import io schema_json = \"\"\" { \"type\": \"record\", \"name\": \"User\", \"fields\": [ {\"name\": \"name\", \"type\": \"string\"}, {\"name\": \"age\", \"type\": \"int\"}, {\"name\": \"email\", \"type\": [\"null\", \"string\"], \"default\": null} ] } \"\"\" schema = avro . schema . parse ( schema_json ) users = [ { \"name\" : \"John Doe\" , \"age\" : 28 , \"email\" : \"john.doe@example.com\" }, { \"name\" : \"Jane Doe\" , \"age\" : 25 , \"email\" : None } ] with open ( \"users.avro\" , \"wb\" ) as avro_file : writer = avro . datafile . DataFileWriter ( avro_file , avro . io . DatumWriter (), schema ) for user in users : writer . append ( user ) writer . close () print ( \"Data written to users.avro\" )","title":"Writing Avro Data to a File:"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/4.Avro/#reading-avro-data-from-a-file","text":"with open ( \"users.avro\" , \"rb\" ) as avro_file : reader = avro . datafile . DataFileReader ( avro_file , avro . io . DatumReader ()) for user in reader : print ( user ) reader . close ()","title":"Reading Avro Data from a File:"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/4.Avro/#output_2","text":"{'name': 'John Doe', 'age': 28, 'email': 'john.doe@example.com'} {'name': 'Jane Doe', 'age': 25, 'email': None}","title":"Output:"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/4.Avro/#4-schema-evolution-example","text":"One of Avro\u2019s powerful features is schema evolution . Let\u2019s say you add a new field to the schema but still want to read data serialized with the old schema.","title":"4. Schema Evolution Example"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/4.Avro/#original-schema-old","text":"{ \"type\" : \"record\" , \"name\" : \"User\" , \"fields\" : [ { \"name\" : \"name\" , \"type\" : \"string\" }, { \"name\" : \"age\" , \"type\" : \"int\" } ] }","title":"Original Schema (Old):"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/4.Avro/#new-schema-evolved","text":"{ \"type\" : \"record\" , \"name\" : \"User\" , \"fields\" : [ { \"name\" : \"name\" , \"type\" : \"string\" }, { \"name\" : \"age\" , \"type\" : \"int\" }, { \"name\" : \"email\" , \"type\" : [ \"null\" , \"string\" ], \"default\" : null } ] }","title":"New Schema (Evolved):"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/4.Avro/#python-example-handling-schema-evolution","text":"If data was written with the old schema, you can still read it with the new schema that has the email field added. new_schema_json = \"\"\" { \"type\": \"record\", \"name\": \"User\", \"fields\": [ {\"name\": \"name\", \"type\": \"string\"}, {\"name\": \"age\", \"type\": \"int\"}, {\"name\": \"email\", \"type\": [\"null\", \"string\"], \"default\": null} ] } \"\"\" new_schema = avro . schema . parse ( new_schema_json ) with open ( \"users_old.avro\" , \"rb\" ) as avro_file : reader = avro . datafile . DataFileReader ( avro_file , avro . io . DatumReader ( new_schema )) for user in reader : print ( user ) reader . close ()","title":"Python Example (Handling Schema Evolution):"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/4.Avro/#output_3","text":"```plaintext {'name': 'John Doe', 'age': 28, 'email': None} {'name': 'Jane Doe', 'age': 25, 'email': None}","title":"Output:"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/5.Falcor/","text":"Falcor is a JavaScript library developed by Netflix It uses JSON Key concepts of Falcor: Virtual JSON Model : Falcor presents your data as a JSON object, even if the actual data is spread across different services or databases. Path-Based Access : Instead of making multiple HTTP requests to various APIs, you access data via paths, much like accessing properties in a JavaScript object. Optimized Data Fetching : Falcor only fetches the data you need, minimizing over-fetching and under-fetching of data. Caching : Falcor has built-in client-side caching, reducing the need for redundant network requests. 1. Setting up a Falcor Model \u00b6 This example shows how to create a Falcor model to interact with a virtual JSON graph. const falcor = require ( 'falcor' ); const HttpDataSource = require ( 'falcor-http-datasource' ); // Creating a model connected to a Falcor server const model = new falcor . Model ({ source : new HttpDataSource ( '/model.json' ) // URL to your Falcor server }); 2. Fetching Data using Path \u00b6 In this example, you fetch data by specifying a path within your virtual JSON model. // Retrieve data from a specific path in the virtual JSON model model . get ([ 'movies' , 1234 , 'title' ]). then ( response => { console . log ( response . json ); // Output: { movies: { 1234: { title: 'The Matrix' } } } }); Here, we are retrieving the title of a movie with ID 1234 by requesting the path ['movies', 1234, 'title'] . 3. Fetching Multiple Values \u00b6 You can request multiple values from different paths in a single call. model . get ( [ 'movies' , 1234 , 'title' ], [ 'movies' , 1234 , 'releaseYear' ], [ 'movies' , 5678 , 'title' ] ). then ( response => { console . log ( response . json ); // Output: // { // movies: { // 1234: { title: 'The Matrix', releaseYear: 1999 }, // 5678: { title: 'Inception' } // } // } }); 4. Setting Data in the Falcor Model \u00b6 You can also update data in the model using set() . model . set ({ path : [ 'movies' , 1234 , 'title' ], value : 'The Matrix Reloaded' }). then ( response => { console . log ( response . json ); // Output: { movies: { 1234: { title: 'The Matrix Reloaded' } } } }); 5. Using Falcor Router on the Server-Side \u00b6 Here\u2019s an example of a simple Falcor Router on the server that serves data. const falcorExpress = require ( 'falcor-express' ); const Router = require ( 'falcor-router' ); const express = require ( 'express' ); const app = express (); // Define the Falcor route app . use ( '/model.json' , falcorExpress . dataSourceRoute (( req , res ) => { return new Router ([ { route : \"movies[{integers:ids}]['title', 'releaseYear']\" , get : ( pathSet ) => { const results = []; pathSet . ids . forEach ( id => { results . push ({ path : [ 'movies' , id , 'title' ], value : id === 1234 ? 'The Matrix' : 'Inception' }); results . push ({ path : [ 'movies' , id , 'releaseYear' ], value : id === 1234 ? 1999 : 2010 }); }); return results ; } } ]); })); app . listen ( 3000 , () => { console . log ( 'Falcor server running on port 3000' ); }); This sets up a Falcor router that responds to requests for movie titles and release years based on the provided movie IDs. 6. Combining Falcor with React \u00b6 You can use Falcor with React to fetch and display data. import React , { useEffect , useState } from 'react' ; import falcor from 'falcor' ; import HttpDataSource from 'falcor-http-datasource' ; const model = new falcor . Model ({ source : new HttpDataSource ( '/model.json' ) }); const MoviesList = () => { const [ movies , setMovies ] = useState ([]); useEffect (() => { model . get ([ 'movies' , [ 1234 , 5678 ], 'title' ]). then ( response => { setMovies ( response . json . movies ); }); }, []); return ( < div > { Object . keys ( movies ). map ( id => ( < div key = { id } > < h3 > { movies [ id ]. title } < /h3> < /div> ))} < /div> ); }; export default MoviesList ; In this example, the MoviesList component fetches movie titles with IDs 1234 and 5678 when the component loads, and displays them in the UI .","title":"5.Falcor"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/5.Falcor/#1-setting-up-a-falcor-model","text":"This example shows how to create a Falcor model to interact with a virtual JSON graph. const falcor = require ( 'falcor' ); const HttpDataSource = require ( 'falcor-http-datasource' ); // Creating a model connected to a Falcor server const model = new falcor . Model ({ source : new HttpDataSource ( '/model.json' ) // URL to your Falcor server });","title":"1. Setting up a Falcor Model"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/5.Falcor/#2-fetching-data-using-path","text":"In this example, you fetch data by specifying a path within your virtual JSON model. // Retrieve data from a specific path in the virtual JSON model model . get ([ 'movies' , 1234 , 'title' ]). then ( response => { console . log ( response . json ); // Output: { movies: { 1234: { title: 'The Matrix' } } } }); Here, we are retrieving the title of a movie with ID 1234 by requesting the path ['movies', 1234, 'title'] .","title":"2. Fetching Data using Path"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/5.Falcor/#3-fetching-multiple-values","text":"You can request multiple values from different paths in a single call. model . get ( [ 'movies' , 1234 , 'title' ], [ 'movies' , 1234 , 'releaseYear' ], [ 'movies' , 5678 , 'title' ] ). then ( response => { console . log ( response . json ); // Output: // { // movies: { // 1234: { title: 'The Matrix', releaseYear: 1999 }, // 5678: { title: 'Inception' } // } // } });","title":"3. Fetching Multiple Values"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/5.Falcor/#4-setting-data-in-the-falcor-model","text":"You can also update data in the model using set() . model . set ({ path : [ 'movies' , 1234 , 'title' ], value : 'The Matrix Reloaded' }). then ( response => { console . log ( response . json ); // Output: { movies: { 1234: { title: 'The Matrix Reloaded' } } } });","title":"4. Setting Data in the Falcor Model"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/5.Falcor/#5-using-falcor-router-on-the-server-side","text":"Here\u2019s an example of a simple Falcor Router on the server that serves data. const falcorExpress = require ( 'falcor-express' ); const Router = require ( 'falcor-router' ); const express = require ( 'express' ); const app = express (); // Define the Falcor route app . use ( '/model.json' , falcorExpress . dataSourceRoute (( req , res ) => { return new Router ([ { route : \"movies[{integers:ids}]['title', 'releaseYear']\" , get : ( pathSet ) => { const results = []; pathSet . ids . forEach ( id => { results . push ({ path : [ 'movies' , id , 'title' ], value : id === 1234 ? 'The Matrix' : 'Inception' }); results . push ({ path : [ 'movies' , id , 'releaseYear' ], value : id === 1234 ? 1999 : 2010 }); }); return results ; } } ]); })); app . listen ( 3000 , () => { console . log ( 'Falcor server running on port 3000' ); }); This sets up a Falcor router that responds to requests for movie titles and release years based on the provided movie IDs.","title":"5. Using Falcor Router on the Server-Side"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/5.Falcor/#6-combining-falcor-with-react","text":"You can use Falcor with React to fetch and display data. import React , { useEffect , useState } from 'react' ; import falcor from 'falcor' ; import HttpDataSource from 'falcor-http-datasource' ; const model = new falcor . Model ({ source : new HttpDataSource ( '/model.json' ) }); const MoviesList = () => { const [ movies , setMovies ] = useState ([]); useEffect (() => { model . get ([ 'movies' , [ 1234 , 5678 ], 'title' ]). then ( response => { setMovies ( response . json . movies ); }); }, []); return ( < div > { Object . keys ( movies ). map ( id => ( < div key = { id } > < h3 > { movies [ id ]. title } < /h3> < /div> ))} < /div> ); }; export default MoviesList ; In this example, the MoviesList component fetches movie titles with IDs 1234 and 5678 when the component loads, and displays them in the UI .","title":"6. Combining Falcor with React"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/8.ActiveMQ/","text":"ActiveMQ is another message broker like 5.kafka it may-be used as pub/sub and message queue (see 4.Pub-sub vs message queue ) It can work over OpenWire STOMP AMQP MQTT HTTP and supports SSL for encryption. Also it can scale from single to complex, multi-broker architectures Here is nice example of Kafka vs RabbitMQ vs ActiveMQ Here is more detailed architecture of ActiveMQ Paging: In ActiveMQ , paging occurs when the broker stores messages on disk after exceeding its memory limit. This allows message producers to continue sending messages without running out of memory. Once memory usage drops, the broker retrieves and delivers the paged messages. Paging helps manage large message volumes by preventing memory exhaustion, and it's configured by adjusting memory and disk usage limits like memoryLimit and storeUsage . Journal - a high-performance, sequential log used to store message data before it's written to more permanent storage, like a database. The journal ensures data durability by recording all incoming messages and transactions, allowing recovery in case of a failure. Core client API . This is a simple intuitive Java API that allows the full set of messaging functionality without some of the complexities of JMS. JMS client API The standard JMS API is available at the client side. JDBC (Java Database Connectivity) is an API in Java that enables applications to interact with relational databases. It provides methods to execute SQL queries, update records, and retrieve data from databases like MySQL, PostgreSQL, and Oracle. More detailed info you can find here https://activemq.apache.org/components/artemis/documentation/1.0.0/index.html","title":"8.ActiveMQ"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/3.Expert%20Part/9.Nats/","text":"Well, Nats is a bit different type, developers themselves call it message oriented middleware (we can say that it is some kind of ESB ) Here you can see a principle of NAT's work As you can observe, it is really simple message queue/ESB In NATS, messages are published to subjects\u2014hierarchical string values that subscribers monitor to receive messages. Known for its high throughput and low latency, NATS supports multiple message formats, including JSON, XML, and plain text. Here is architecture So, what are pros ? Lightweight and High Performance : Delivers excellent performance with minimal resource consumption. Delivery Guarantees : Provides both at-most-once and at-least-once messaging semantics. User-Friendly : Easy to deploy and manage, without requiring a dedicated cluster. Scalable : Designed to scale efficiently in distributed systems and microservices architectures.","title":"9.Nats"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/4.Cases/1.POST%20as%20GET/","text":"","title":"1.POST as GET"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/1.Integrations/5.Extra%20materials/0.%20Extra%20Materials/","text":"https://stackoverflow.com/questions/38024514/understanding-kafka-topics-and-partitions https://medium.com/javarevisited/kafka-partitions-and-consumer-groups-in-6-mins-9e0e336c6c00 https://redpanda.com/what-is-redpanda https://netflix.github.io/falcor/starter/what-is-falcor.html https://medium.com/javarevisited/difference-between-rabbitmq-apache-kafka-and-activemq-65e26b923114 https://medium.com/another-integration-blog/what-is-an-enterprise-service-bus-1991646764e2 https://eda-visuals.boyney.io/visuals/queues-vs-streams-vs-pubsub","title":"0. Extra Materials"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/2.Logic%20Separation/0.Introduction/","text":"Every application you has its own set of logic elements Authorisation Notification etc. Besides the fact that it is one application, all those logic parts may Be requested with different frequencies Require different data Be implemented with different technologies Require different states etc. When those restrictions are overwhelming, we can thing towards separating some parts or even creating separate applications. In a nutshell, this part will be dedicated to different ways logic can be (or should not be) separated.","title":"0.Introduction"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/2.Logic%20Separation/1.Monolith/","text":"When you start creating any application, you should not make it difficult. You need to make it work right now, so you start writing code put everything into one application. Bravo ! You have just created a Monolithic Architecture. How? Well, before going deeper into monolith let's define what is it ? The most basic and strict characteristics of a monolith architecture may be squeezed down to the following points - Single executable program - Single instance - One database (we talk about one database of each type ) - Unified Build and Deployment - Communication within application Is it a good or a bad approach? This question is not right. Each approach has its own pros and cons, so let's list them Advantages Disadvantages The simplicity of starting to develop Undoubtedly, it is easy to create a couple of classes with an array of functions in each that complete some uncomplicated operations. Hard to support If the error is not a simple one like \u201cline 147 pages.py method length() is not callable\u201d, but anything more complicated, it is almost impossible to find the right place, where error occurred. Moreover, when a program does not fail, but just gives wrong results, you may face a vast array of difficulties trying to figure out part of the system, where it started to fail as everything is messed up and tightly coupled The easiness of testing Besides being the \u00abfirst\u00bb architecture design, it is without any doubt the one that provides the simplest way of testing, especially e2e testing as the only thing that is required is to run the program Whole system redeployment As a monolithic application is to be redeployed wholly, when one part fails or a new release is coming out, such a routine may drive a lot of people mad, and at the same time it has a severe effect on developing and down time No difficulties in deployment Almost always in such types of applications the only necessary thing is to write commands \u201cyarn start\u201d or \u201cpython file.py\u201d without any other parameters to configure. Parallel work is challenging As the system is tightly coupled, hardly can anyone argue that one part can be changed while the other is not finished, which makes projects/products that are based on this architecture design very slow and not let them use agile methodologies Moreover, when we talk about architecture and programming, it is vital to get into understanding two following things Cohesion and Coupling Why? Because usually bad monoliths are ones that have low cohesion, which led at some moment to appearing of new architecture styles that we will discuss in the following parts. For now we will stop here, however, later in more detailed version (so, when I have time), we will dive deeper into monolith and its types","title":"1.Monolith"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/2.Logic%20Separation/2.SOA/","text":"Monolith is nice, it is easy to start, deploy and test and many other things. NB: we will compare all the approaches by the end of the section However, it is also may be a bit challenging when your product grows and absorbs more and more functionality, since more and more logical modules appear. This leads to increasing difficulty of New modules integrations Support of the legacy At this point people started to think about possibility to separate logical modules one from another, making them more independent. This would allow to work on any given module without interference with others. For example, we can distinguish billing service from the rest of the application since We may need to adjust it to PCI DSS It is called from every other module with different parameters and we would like to create universal interface It is update really often and we don't want to redeploy the whole application every time. As a result, SOA has appeared. SOA stands for Service Oriented Architecture But what does it mean ? Let's look at a picture, I've stoled from some site a long time ago, while writing my thesis back in 2022 On this diagram you may witness the most common example of SOA, let's break it down Software components are divided into services Service is responsible for specific functionality and should perform without knowing anything about other services or system context Services are reusable Services are usually integrated with presentation layer via Enterprise Service Bus Services are accessible via network protocols such as XML/HTTP or JSON/HTTP Services may be built from scratch, but usually are used as imported functions from legacy systems Shared database Also, besides al those facts I'd like to highlight one more, which may spark some debates Repetitive code is fine, we just make ctrl + c , ctrl + v Why? Because in this approach we focus not on separating logical code parts, but on business side of the process. Meaning that we can freely have the same code in the different services. Let's dive deeper into advantages and disadvantages Advantages Drawbacks Faster time to market (Big companies) As reusability is one of the main axioms in the SOA paradigm, importing existing functions and/or services that were implemented previously would cause a reduction of development time of new products. High cost Besides being easily supported, the process of setting up a SOA project is irrevocably linked with giant monetary investments into technologies, development, and human resources. Such a curse is dictated by the need of specialists with deep expertise and a vast array of specific technological advances Parts independence Behind being one of the most desirable characteristics of an application in programmers\u2019 world, such a point plays a pivotal role in getting rid of a ripple effect and all other consequences of tight coupling. Need in high bandwidth server As almost all web-services send and receive an enormous amount of information frequently, the enterprise service bus is to be run on a very powerful workstation that will be capable of routing all those millions of requests. Easy maintenance There is no need to keep in mind all connections as in monolithic one, as well it is not necessary to track all logic as every single service is responsible only for one business part. Low reliability of internet protocols The Internet network is accountable for communication between services, unfortunately it does not always work in a proper way in order to fulfill customers\u2019 and developers\u2019 needs, just as sporadically occur some receiving or sending issues connected with time. Scalability Several instances of a service can be launched on different machines. Improved collaboration between IT and business Since every service is accountable for only one business task, business-analyst is able to work in a more efficient way with developers\u2019 team as he or she may now concentrate on business insights not on tracking all connections inside an application","title":"2.SOA"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/2.Logic%20Separation/3.MSA/","text":"","title":"3.MSA"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/2.Logic%20Separation/4.EDA/","text":"","title":"4.EDA"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/2.Logic%20Separation/5.DDD/","text":"","title":"5.DDD"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/2.Logic%20Separation/CQRS/","text":"","title":"CQRS"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/2.Logic%20Separation/SAGA/","text":"","title":"SAGA"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/3.Best%20Practices/3.Best%20practices/","text":"Microservices' structuring \u00b6 Methods naming \u00b6 Creating methods response logic \u00b6 Updating methods response logic \u00b6","title":"Microservices' structuring"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/3.Best%20Practices/3.Best%20practices/#microservices-structuring","text":"","title":"Microservices' structuring"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/3.Best%20Practices/3.Best%20practices/#methods-naming","text":"","title":"Methods naming"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/3.Best%20Practices/3.Best%20practices/#creating-methods-response-logic","text":"","title":"Creating methods response logic"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/3.Best%20Practices/3.Best%20practices/#updating-methods-response-logic","text":"","title":"Updating methods response logic"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/4.Authorisation/0.%20Authentication%20vs%20Authorisation/","text":"","title":"0. Authentication vs Authorisation"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/99.Additional%20Materials/1.%20Cohesion%20and%20Coupeling/","text":"Cohesion \u00b6 Cohesion refers to how closely related and focused the responsibilities of a single module, class, or function are. Basically, this term refers to the measure how different parts of the same module is linked to each other - High cohesion : Modules or classes have a focused, single responsibility, making the code easier to understand, maintain, and modify. For example, a class that manages user authentication should only deal with tasks related to authentication (e.g., logging in, logging out, checking credentials). - Low cohesion : Modules or classes handle unrelated tasks, making the code more complex and harder to maintain. For example, a class that manages both user authentication and payment processing has low cohesion, because these are distinct responsibilities Coupling \u00b6 Coupling refers to the degree of direct dependence between different modules or components in a system. This measure evaluates how different parts of one module is connected with another module - Low coupling : Modules or components are independent and interact with each other through well-defined interfaces. This makes the system more flexible, allowing you to change or replace one part without significantly affecting others. - High coupling : Modules are tightly dependent on each other, which makes changes in one module likely to cause issues in others. This can lead to maintenance problems and make it harder to scale or refactor the system. So, we always should try to aim high cohesion low coupling","title":"1. Cohesion and Coupeling"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/99.Additional%20Materials/1.%20Cohesion%20and%20Coupeling/#cohesion","text":"Cohesion refers to how closely related and focused the responsibilities of a single module, class, or function are. Basically, this term refers to the measure how different parts of the same module is linked to each other - High cohesion : Modules or classes have a focused, single responsibility, making the code easier to understand, maintain, and modify. For example, a class that manages user authentication should only deal with tasks related to authentication (e.g., logging in, logging out, checking credentials). - Low cohesion : Modules or classes handle unrelated tasks, making the code more complex and harder to maintain. For example, a class that manages both user authentication and payment processing has low cohesion, because these are distinct responsibilities","title":"Cohesion"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/99.Additional%20Materials/1.%20Cohesion%20and%20Coupeling/#coupling","text":"Coupling refers to the degree of direct dependence between different modules or components in a system. This measure evaluates how different parts of one module is connected with another module - Low coupling : Modules or components are independent and interact with each other through well-defined interfaces. This makes the system more flexible, allowing you to change or replace one part without significantly affecting others. - High coupling : Modules are tightly dependent on each other, which makes changes in one module likely to cause issues in others. This can lead to maintenance problems and make it harder to scale or refactor the system. So, we always should try to aim high cohesion low coupling","title":"Coupling"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/99.Additional%20Materials/2.%20Hinted%20Handoff/","text":"In simple terms, Hinted Handoff is a clever way for distributed systems to handle write operations even when some of the nodes in the system are temporarily unavailable. It ensures that data isn\u2019t lost and the system can recover smoothly when everything is back online. How Does Hinted Handoff Work? \u00b6 Let\u2019s break it down step by step: Writing Data When Nodes Are Down Imagine you\u2019re writing data, but some of the nodes responsible for storing that data are unreachable\u2014maybe they\u2019re down or have network issues. Instead of failing the write, another available node steps in and stores the data temporarily. Along with the data, it keeps a \u201c hint \u201d\u2014basically a note saying: \u201cHey, this data actually belongs to Node X, but I\u2019ll hold onto it for now.\u201d Holding the Data Safely The available node holds onto the data and the hint until the unavailable node (Node X) comes back online. Reconciliation: Delivering the Data Once Node X comes back, the node holding the hint reaches out and transfers the data to it. This process updates Node X with the correct data, and the hint is cleared from the temporary node. Getting Back to Normal Eventually, all nodes are up-to-date, and the system is back in a consistent state. Why Is Hinted Handoff Useful? \u00b6 Hinted Handoff comes with several benefits that make it a lifesaver in distributed systems: Writes Always Work : Even if some nodes are down, the system can still accept and store writes. That means fewer interruptions and better availability. No Data Loss : By temporarily holding the data, Hinted Handoff ensures that nothing gets lost, even if some parts of the system fail. Eventual Consistency : Once the nodes come back online, the system syncs up and all nodes have the correct data. It may take a little time, but consistency is eventually guaranteed. Handles Failures Gracefully : Systems can bounce back after temporary failures without needing a complex recovery process. Automatic Recovery : As soon as nodes are back, the reconciliation process happens automatically\u2014no manual fixes needed. But It\u2019s Not Perfect \u00b6 Of course, there are some downsides to using Hinted Handoff: Temporary Inconsistency : Until the unavailable nodes are updated, some parts of the system might have outdated data. For applications that require \u201cperfect\u201d consistency at all times, this can be a problem. Resource Usage : The nodes storing temporary data and hints need extra resources like storage and CPU, which can add load to the system. More Complexity : Implementing and managing hints adds some complexity to the system design. Risk of Losing Data : If the node holding the hint crashes before it can transfer the data, there\u2019s a risk of losing that information (though replication strategies can help reduce this risk). Delays : Synchronizing everything back to normal can take time, which may not be ideal for real-time systems that need instant consistency. How to Visualize Hinted Handoff? \u00b6 Here\u2019s a simple picture to help you understand: Step 1: Node Failure Node A tries to write data to Node X (unavailable). Node A temporarily stores the data and a hint for Node X. Step 2: Holding the Data Node A keeps the data safe and waits for Node X to come back online. Step 3: Reconciliation When Node X is back, Node A delivers the data along with the hint. Node X updates its storage, and everything syncs up. Example Visualization \u00b6 If you\u2019re looking for an illustration, here\u2019s a simple conceptual image: Diagram: \u00b6 Node A \u2192 Writes data intended for Node X . Node A holds: Data: Some important data Hint: \u201cFor Node X\u201d Once Node X is back: Node A sends the data \u2192 Node X stores it. Hint cleared.","title":"2. Hinted Handoff"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/99.Additional%20Materials/2.%20Hinted%20Handoff/#how-does-hinted-handoff-work","text":"Let\u2019s break it down step by step: Writing Data When Nodes Are Down Imagine you\u2019re writing data, but some of the nodes responsible for storing that data are unreachable\u2014maybe they\u2019re down or have network issues. Instead of failing the write, another available node steps in and stores the data temporarily. Along with the data, it keeps a \u201c hint \u201d\u2014basically a note saying: \u201cHey, this data actually belongs to Node X, but I\u2019ll hold onto it for now.\u201d Holding the Data Safely The available node holds onto the data and the hint until the unavailable node (Node X) comes back online. Reconciliation: Delivering the Data Once Node X comes back, the node holding the hint reaches out and transfers the data to it. This process updates Node X with the correct data, and the hint is cleared from the temporary node. Getting Back to Normal Eventually, all nodes are up-to-date, and the system is back in a consistent state.","title":"How Does Hinted Handoff Work?"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/99.Additional%20Materials/2.%20Hinted%20Handoff/#why-is-hinted-handoff-useful","text":"Hinted Handoff comes with several benefits that make it a lifesaver in distributed systems: Writes Always Work : Even if some nodes are down, the system can still accept and store writes. That means fewer interruptions and better availability. No Data Loss : By temporarily holding the data, Hinted Handoff ensures that nothing gets lost, even if some parts of the system fail. Eventual Consistency : Once the nodes come back online, the system syncs up and all nodes have the correct data. It may take a little time, but consistency is eventually guaranteed. Handles Failures Gracefully : Systems can bounce back after temporary failures without needing a complex recovery process. Automatic Recovery : As soon as nodes are back, the reconciliation process happens automatically\u2014no manual fixes needed.","title":"Why Is Hinted Handoff Useful?"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/99.Additional%20Materials/2.%20Hinted%20Handoff/#but-its-not-perfect","text":"Of course, there are some downsides to using Hinted Handoff: Temporary Inconsistency : Until the unavailable nodes are updated, some parts of the system might have outdated data. For applications that require \u201cperfect\u201d consistency at all times, this can be a problem. Resource Usage : The nodes storing temporary data and hints need extra resources like storage and CPU, which can add load to the system. More Complexity : Implementing and managing hints adds some complexity to the system design. Risk of Losing Data : If the node holding the hint crashes before it can transfer the data, there\u2019s a risk of losing that information (though replication strategies can help reduce this risk). Delays : Synchronizing everything back to normal can take time, which may not be ideal for real-time systems that need instant consistency.","title":"But It\u2019s Not Perfect"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/99.Additional%20Materials/2.%20Hinted%20Handoff/#how-to-visualize-hinted-handoff","text":"Here\u2019s a simple picture to help you understand: Step 1: Node Failure Node A tries to write data to Node X (unavailable). Node A temporarily stores the data and a hint for Node X. Step 2: Holding the Data Node A keeps the data safe and waits for Node X to come back online. Step 3: Reconciliation When Node X is back, Node A delivers the data along with the hint. Node X updates its storage, and everything syncs up.","title":"How to Visualize Hinted Handoff?"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/99.Additional%20Materials/2.%20Hinted%20Handoff/#example-visualization","text":"If you\u2019re looking for an illustration, here\u2019s a simple conceptual image:","title":"Example Visualization"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/99.Additional%20Materials/2.%20Hinted%20Handoff/#diagram","text":"Node A \u2192 Writes data intended for Node X . Node A holds: Data: Some important data Hint: \u201cFor Node X\u201d Once Node X is back: Node A sends the data \u2192 Node X stores it. Hint cleared.","title":"Diagram:"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/99.Additional%20Materials/99.Additional%20Materials/","text":"","title":"99.Additional Materials"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/99.Additional%20Materials/999.Additional%20Materials/","text":"","title":"999.Additional Materials"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/99.Additional%20Materials/Lamport%20Timestamps%20vs%20Vector%20Clocks/","text":"Lamport Timestamps and Vector Clocks are both logical clock mechanisms used in distributed systems to order events, but they serve different purposes and have different properties. 1. Lamport Timestamps \u00b6 Concept: Introduced by Leslie Lamport in 1978, Lamport timestamps provide a way to order events in a distributed system. Each process maintains a counter (logical clock), which increments before sending a message. When a process receives a message, it updates its counter to be greater than both its current value and the timestamp in the received message. Algorithm: Each process maintains a counter ( L ), initialized to 0. Before an event (like sending a message), the process increments L . When sending a message, it includes L as a timestamp. Upon receiving a message, the process updates its counter to max(L_receiver, L_sender + 1) . Pros: Ensures a partial ordering of events. Simple and efficient (single integer per process). Cons: Cannot determine causal relationships precisely\u2014two events may have the same timestamp but be independent. Does not detect concurrency (i.e., whether two events happened simultaneously in different processes). 2. Vector Clocks \u00b6 Concept: Vector clocks extend Lamport timestamps to capture causality more precisely. Instead of a single counter, each process maintains a vector of counters (one for each process). Algorithm: Each process maintains a vector clock ( V ), initialized to [0,0,...,0] (one entry per process). Before an event, the process increments its own counter ( V[i] += 1 ). When sending a message, the entire vector clock is included. Upon receiving a message, the recipient updates its vector clock by taking the element-wise maximum and incrementing its own entry: V_receiver[j] = max(V_receiver[j], V_sender[j]) for all j . V_receiver[i] += 1 (for its own index). Pros: Captures causality accurately. Can determine if two events are concurrent ( A || B if neither dominates the other). Cons: Requires more storage and computation (O(N) space and comparison). More complex than Lamport timestamps. Comparison Summary \u00b6 Feature Lamport Timestamps Vector Clocks Order Events Yes (partial order) Yes (total order if needed) Causal Relationship No Yes Concurrency Detection No Yes Storage Complexity O(1) per process O(N) per process Computation Overhead Low Higher Example: A Distributed System with Three Processes \u00b6 P1 sends a message to P2. P2 sends a message to P3. P1 and P3 perform independent events. 1. Using Lamport Timestamps \u00b6 Each process maintains a single integer timestamp. Event Description P1 P2 P3 E1 P1 executes an event 1 0 0 E2 P1 sends a message to P2 2 0 0 E3 P2 receives message from P1 2 3 0 E4 P2 sends a message to P3 2 4 0 E5 P3 receives message from P2 2 4 5 E6 P1 executes an independent event 3 4 5 E7 P3 executes an independent event 3 4 6 Observations (Lamport Timestamps) \u00b6 The timestamps only order events . Causal relationships are not explicit . E2 (P1 \u2192 P2) happened before E3 because 2 < 3. However, E6 (P1's independent event) and E7 (P3's independent event) have no causal link but are still assigned timestamps . 2. Using Vector Clocks \u00b6 Each process maintains a vector clock [P1, P2, P3] . Event Description P1 P2 P3 E1 P1 executes an event [1,0,0] [0,0,0] [0,0,0] E2 P1 sends a message to P2 [2,0,0] [0,0,0] [0,0,0] E3 P2 receives message from P1 [2,0,0] [2,1,0] [0,0,0] E4 P2 sends a message to P3 [2,0,0] [2,2,0] [0,0,0] E5 P3 receives message from P2 [2,0,0] [2,2,0] [2,2,1] E6 P1 executes an independent event [3,0,0] [2,2,0] [2,2,1] E7 P3 executes an independent event [3,0,0] [2,2,0] [2,2,2] Observations (Vector Clocks) \u00b6 Captures causality : E3 [2,1,0] happens after E2 [2,0,0] because 2 \u2265 2, 1 > 0, 0 = 0 . E5 [2,2,1] happens after E4 [2,2,0] . Detects concurrency : E6 [3,0,0] (P1) and E7 [2,2,2] (P3) are concurrent because neither vector is greater than the other. This means E6 and E7 happened independently without causal relation. Key Differences in the Example \u00b6 Feature Lamport Timestamps Vector Clocks Event Ordering Yes Yes Causal Relationships No Yes Concurrency Detection No Yes Storage Cost O(1) per process O(N) per process Final Thoughts \u00b6 Lamport Timestamps : When you only need ordering, not causality (e.g., mutual exclusion, distributed snapshots). Vector Clocks : When causal relationships matter (e.g., version control, event tracking in distributed databases).","title":"Lamport Timestamps vs Vector Clocks"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/99.Additional%20Materials/Lamport%20Timestamps%20vs%20Vector%20Clocks/#1-lamport-timestamps","text":"Concept: Introduced by Leslie Lamport in 1978, Lamport timestamps provide a way to order events in a distributed system. Each process maintains a counter (logical clock), which increments before sending a message. When a process receives a message, it updates its counter to be greater than both its current value and the timestamp in the received message. Algorithm: Each process maintains a counter ( L ), initialized to 0. Before an event (like sending a message), the process increments L . When sending a message, it includes L as a timestamp. Upon receiving a message, the process updates its counter to max(L_receiver, L_sender + 1) . Pros: Ensures a partial ordering of events. Simple and efficient (single integer per process). Cons: Cannot determine causal relationships precisely\u2014two events may have the same timestamp but be independent. Does not detect concurrency (i.e., whether two events happened simultaneously in different processes).","title":"1. Lamport Timestamps"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/99.Additional%20Materials/Lamport%20Timestamps%20vs%20Vector%20Clocks/#2-vector-clocks","text":"Concept: Vector clocks extend Lamport timestamps to capture causality more precisely. Instead of a single counter, each process maintains a vector of counters (one for each process). Algorithm: Each process maintains a vector clock ( V ), initialized to [0,0,...,0] (one entry per process). Before an event, the process increments its own counter ( V[i] += 1 ). When sending a message, the entire vector clock is included. Upon receiving a message, the recipient updates its vector clock by taking the element-wise maximum and incrementing its own entry: V_receiver[j] = max(V_receiver[j], V_sender[j]) for all j . V_receiver[i] += 1 (for its own index). Pros: Captures causality accurately. Can determine if two events are concurrent ( A || B if neither dominates the other). Cons: Requires more storage and computation (O(N) space and comparison). More complex than Lamport timestamps.","title":"2. Vector Clocks"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/99.Additional%20Materials/Lamport%20Timestamps%20vs%20Vector%20Clocks/#comparison-summary","text":"Feature Lamport Timestamps Vector Clocks Order Events Yes (partial order) Yes (total order if needed) Causal Relationship No Yes Concurrency Detection No Yes Storage Complexity O(1) per process O(N) per process Computation Overhead Low Higher","title":"Comparison Summary"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/99.Additional%20Materials/Lamport%20Timestamps%20vs%20Vector%20Clocks/#example-a-distributed-system-with-three-processes","text":"P1 sends a message to P2. P2 sends a message to P3. P1 and P3 perform independent events.","title":"Example: A Distributed System with Three Processes"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/99.Additional%20Materials/Lamport%20Timestamps%20vs%20Vector%20Clocks/#1-using-lamport-timestamps","text":"Each process maintains a single integer timestamp. Event Description P1 P2 P3 E1 P1 executes an event 1 0 0 E2 P1 sends a message to P2 2 0 0 E3 P2 receives message from P1 2 3 0 E4 P2 sends a message to P3 2 4 0 E5 P3 receives message from P2 2 4 5 E6 P1 executes an independent event 3 4 5 E7 P3 executes an independent event 3 4 6","title":"1. Using Lamport Timestamps"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/99.Additional%20Materials/Lamport%20Timestamps%20vs%20Vector%20Clocks/#observations-lamport-timestamps","text":"The timestamps only order events . Causal relationships are not explicit . E2 (P1 \u2192 P2) happened before E3 because 2 < 3. However, E6 (P1's independent event) and E7 (P3's independent event) have no causal link but are still assigned timestamps .","title":"Observations (Lamport Timestamps)"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/99.Additional%20Materials/Lamport%20Timestamps%20vs%20Vector%20Clocks/#2-using-vector-clocks","text":"Each process maintains a vector clock [P1, P2, P3] . Event Description P1 P2 P3 E1 P1 executes an event [1,0,0] [0,0,0] [0,0,0] E2 P1 sends a message to P2 [2,0,0] [0,0,0] [0,0,0] E3 P2 receives message from P1 [2,0,0] [2,1,0] [0,0,0] E4 P2 sends a message to P3 [2,0,0] [2,2,0] [0,0,0] E5 P3 receives message from P2 [2,0,0] [2,2,0] [2,2,1] E6 P1 executes an independent event [3,0,0] [2,2,0] [2,2,1] E7 P3 executes an independent event [3,0,0] [2,2,0] [2,2,2]","title":"2. Using Vector Clocks"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/99.Additional%20Materials/Lamport%20Timestamps%20vs%20Vector%20Clocks/#observations-vector-clocks","text":"Captures causality : E3 [2,1,0] happens after E2 [2,0,0] because 2 \u2265 2, 1 > 0, 0 = 0 . E5 [2,2,1] happens after E4 [2,2,0] . Detects concurrency : E6 [3,0,0] (P1) and E7 [2,2,2] (P3) are concurrent because neither vector is greater than the other. This means E6 and E7 happened independently without causal relation.","title":"Observations (Vector Clocks)"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/99.Additional%20Materials/Lamport%20Timestamps%20vs%20Vector%20Clocks/#key-differences-in-the-example","text":"Feature Lamport Timestamps Vector Clocks Event Ordering Yes Yes Causal Relationships No Yes Concurrency Detection No Yes Storage Cost O(1) per process O(N) per process","title":"Key Differences in the Example"},{"location":"Courses/Application%20Architecture/1.%20Distributed%20Architecture/99.Additional%20Materials/Lamport%20Timestamps%20vs%20Vector%20Clocks/#final-thoughts","text":"Lamport Timestamps : When you only need ordering, not causality (e.g., mutual exclusion, distributed snapshots). Vector Clocks : When causal relationships matter (e.g., version control, event tracking in distributed databases).","title":"Final Thoughts"},{"location":"Courses/Application%20Architecture/2.Data/Data%20Vault/","text":"","title":"Data Vault"},{"location":"Courses/Application%20Architecture/2.Data/Hadoop/","text":"","title":"Hadoop"},{"location":"Courses/Application%20Architecture/2.Data/1.Database%20Types/0.%2012%20Database%20types/","text":"Relational Database \u00b6 Highlight: Excellent for structured data and complex queries, ensuring data integrity. Use Cases: Ideal for banking, CRM, and any scenario requiring strong ACID compliance. Examples: MySQL, PostgreSQL, Oracle. NoSQL Database \u00b6 Highlight: Great for scalability and flexibility with unstructured data. Use Cases: Suitable for big data analytics, real-time web apps, and content management. Examples: MongoDB, Cassandra, Redis. NewSQL Database \u00b6 Highlight: Combines traditional RDBMS ACID compliance with the scalability of NoSQL. Use Cases: Best for applications needing high transaction rates along with strong consistency, like financial trading platforms and high-speed retail systems. Examples: Google Spanner, CockroachDB, VoltDB. Document-Oriented Database \u00b6 Highlight: Stores data in document formats, offering schema flexibility. Use Cases: Best for content management systems, e-commerce platforms, and applications requiring frequent updates to the data structure. Examples: MongoDB, CouchDB, Amazon DocumentDB. Key-Value Database \u00b6 Highlight: Simple, efficient, and designed for high-speed read and write operations. Use Cases: Session management, caching, and scenarios where quick lookups are critical. Examples: Redis, DynamoDB, Etcd. Column-Oriented Database \u00b6 Highlight: Optimized for reading and writing data in columns, enhancing analytics and query performance. Use Cases: Big data processing, real-time analytics, and data warehousing. Examples: Cassandra, HBase, Google Bigtable. Object-Oriented Database \u00b6 Highlight: Aligns closely with object-oriented programming concepts, storing data as objects. Use Cases: Complex data models like CAD systems, AI applications, and simulation systems. Examples: db4o, ObjectDB, Versant. Time-Series Database \u00b6 Highlight: Specialized in handling time-stamped data, efficient in querying time-based data. Use Cases: IoT applications, financial services, and monitoring systems. Examples: InfluxDB, TimescaleDB, Kdb+. Wide-Column Store \u00b6 Highlight: Combines elements of relational and NoSQL, efficient for storing large volumes of data. Data warehousing, big data processing, and real-time analytics. Examples: Cassandra, Google Bigtable. Spatial Database \u00b6 Highlight: Specialized in storing and querying spatial information like maps and geographic locations. Ideal for geographic information systems (GIS), location-based services, and environmental modeling. Examples: PostGIS (extension for PostgreSQL), Oracle Spatial Graph Database \u00b6 Highlight: Optimized for storing and navigating complex relationships between data points. Use Cases: Social networks, recommendation engines, and fraud detection systems. Examples: Neo4j, Amazon Neptune, OrientDB. in-Memory Database \u00b6 Highlight: Stores data in the main memory (RAM) for faster processing speeds. Use Cases: High-performance applications like telecommunications, gaming, and real-time analytics. Examples: Redis, MemSQL.","title":"Relational Database"},{"location":"Courses/Application%20Architecture/2.Data/1.Database%20Types/0.%2012%20Database%20types/#relational-database","text":"Highlight: Excellent for structured data and complex queries, ensuring data integrity. Use Cases: Ideal for banking, CRM, and any scenario requiring strong ACID compliance. Examples: MySQL, PostgreSQL, Oracle.","title":"Relational Database"},{"location":"Courses/Application%20Architecture/2.Data/1.Database%20Types/0.%2012%20Database%20types/#nosql-database","text":"Highlight: Great for scalability and flexibility with unstructured data. Use Cases: Suitable for big data analytics, real-time web apps, and content management. Examples: MongoDB, Cassandra, Redis.","title":"NoSQL Database"},{"location":"Courses/Application%20Architecture/2.Data/1.Database%20Types/0.%2012%20Database%20types/#newsql-database","text":"Highlight: Combines traditional RDBMS ACID compliance with the scalability of NoSQL. Use Cases: Best for applications needing high transaction rates along with strong consistency, like financial trading platforms and high-speed retail systems. Examples: Google Spanner, CockroachDB, VoltDB.","title":"NewSQL Database"},{"location":"Courses/Application%20Architecture/2.Data/1.Database%20Types/0.%2012%20Database%20types/#document-oriented-database","text":"Highlight: Stores data in document formats, offering schema flexibility. Use Cases: Best for content management systems, e-commerce platforms, and applications requiring frequent updates to the data structure. Examples: MongoDB, CouchDB, Amazon DocumentDB.","title":"Document-Oriented Database"},{"location":"Courses/Application%20Architecture/2.Data/1.Database%20Types/0.%2012%20Database%20types/#key-value-database","text":"Highlight: Simple, efficient, and designed for high-speed read and write operations. Use Cases: Session management, caching, and scenarios where quick lookups are critical. Examples: Redis, DynamoDB, Etcd.","title":"Key-Value Database"},{"location":"Courses/Application%20Architecture/2.Data/1.Database%20Types/0.%2012%20Database%20types/#column-oriented-database","text":"Highlight: Optimized for reading and writing data in columns, enhancing analytics and query performance. Use Cases: Big data processing, real-time analytics, and data warehousing. Examples: Cassandra, HBase, Google Bigtable.","title":"Column-Oriented Database"},{"location":"Courses/Application%20Architecture/2.Data/1.Database%20Types/0.%2012%20Database%20types/#object-oriented-database","text":"Highlight: Aligns closely with object-oriented programming concepts, storing data as objects. Use Cases: Complex data models like CAD systems, AI applications, and simulation systems. Examples: db4o, ObjectDB, Versant.","title":"Object-Oriented Database"},{"location":"Courses/Application%20Architecture/2.Data/1.Database%20Types/0.%2012%20Database%20types/#time-series-database","text":"Highlight: Specialized in handling time-stamped data, efficient in querying time-based data. Use Cases: IoT applications, financial services, and monitoring systems. Examples: InfluxDB, TimescaleDB, Kdb+.","title":"Time-Series Database"},{"location":"Courses/Application%20Architecture/2.Data/1.Database%20Types/0.%2012%20Database%20types/#wide-column-store","text":"Highlight: Combines elements of relational and NoSQL, efficient for storing large volumes of data. Data warehousing, big data processing, and real-time analytics. Examples: Cassandra, Google Bigtable.","title":"Wide-Column Store"},{"location":"Courses/Application%20Architecture/2.Data/1.Database%20Types/0.%2012%20Database%20types/#spatial-database","text":"Highlight: Specialized in storing and querying spatial information like maps and geographic locations. Ideal for geographic information systems (GIS), location-based services, and environmental modeling. Examples: PostGIS (extension for PostgreSQL), Oracle Spatial","title":"Spatial Database"},{"location":"Courses/Application%20Architecture/2.Data/1.Database%20Types/0.%2012%20Database%20types/#graph-database","text":"Highlight: Optimized for storing and navigating complex relationships between data points. Use Cases: Social networks, recommendation engines, and fraud detection systems. Examples: Neo4j, Amazon Neptune, OrientDB.","title":"Graph Database"},{"location":"Courses/Application%20Architecture/2.Data/1.Database%20Types/0.%2012%20Database%20types/#in-memory-database","text":"Highlight: Stores data in the main memory (RAM) for faster processing speeds. Use Cases: High-performance applications like telecommunications, gaming, and real-time analytics. Examples: Redis, MemSQL.","title":"in-Memory Database"},{"location":"Courses/Application%20Architecture/2.Data/999.Additional%20info/ACID/","text":"Atomicity All the writes in a transaction are executed as a single unit and cannot be divided into smaller parts. If any error occurs during the transaction, all changes are undone, ensuring that either all the writes are applied or none at all. This is why atomicity is often described as \"all or nothing.\" Consistency In this context, consistency refers to maintaining the integrity of the database by ensuring that all data adheres to the predefined rules and constraints. Unlike the CAP theorem's concept of consistency, which focuses on reading the latest write or returning an error, here it means that every transaction must leave the database in a valid state. Isolation When multiple transactions are running concurrently, isolation ensures that the operations of one transaction do not interfere with another. The strictest form of isolation is \"serializability,\" where each transaction behaves as though it is the only one being processed. Although serializability is difficult to achieve in practice, more relaxed isolation levels are commonly used. Durability Once a transaction is successfully committed, the data is guaranteed to be stored permanently, even in the event of a system failure. In distributed systems, durability often involves replicating the data across multiple nodes to ensure it remains intact.","title":"ACID"},{"location":"Courses/Application%20Architecture/2.Data/999.Additional%20info/BLOB/","text":"Blob storage is a method of storing large, unstructured data . A \"blob\" represents a chunk of binary data without any specific format or structure. This form of storage is a subset of object storage , which organizes data into a flat, non-hierarchical structure often referred to as a \"data lake\" or \"data pool.\" Object storage differs significantly from traditional approaches like file and block storage: File storage organizes data within directories and folders in a structured hierarchy. Block storage divides data into fixed-size chunks called \"blocks.\" While file and block storage systems may struggle to scale effectively, object storage provides virtually limitless capacity, making it ideal for certain modern needs. However, accessing data within object storage may present additional complexities compared to file or block-based systems. Benefits of this storage model \u00b6 Scalability: It offers expansive storage capabilities that can accommodate growing data volumes without performance degradation. Cloud integration: As a cloud-native solution, it is accessible from anywhere via the Internet, aligning well with cloud-centric operations. Flexibility with programming languages: Developers can use various programming languages to interact with stored data, enhancing accessibility. Cost efficiency: Pricing structures typically include tiers, allowing infrequently accessed data to be stored at lower costs. Best use cases \u00b6 Some notable applications include: Media assets: Storing images, videos, and audio files, especially when access needs are sporadic. Event logs: Preserving the continuous stream of system-generated data for analysis, though cost considerations apply for frequent data queries. Backup solutions: Providing storage for backups and disaster recovery scenarios where data is rarely accessed but must remain available for emergencies.","title":"BLOB"},{"location":"Courses/Application%20Architecture/2.Data/999.Additional%20info/BLOB/#benefits-of-this-storage-model","text":"Scalability: It offers expansive storage capabilities that can accommodate growing data volumes without performance degradation. Cloud integration: As a cloud-native solution, it is accessible from anywhere via the Internet, aligning well with cloud-centric operations. Flexibility with programming languages: Developers can use various programming languages to interact with stored data, enhancing accessibility. Cost efficiency: Pricing structures typically include tiers, allowing infrequently accessed data to be stored at lower costs.","title":"Benefits of this storage model"},{"location":"Courses/Application%20Architecture/2.Data/999.Additional%20info/BLOB/#best-use-cases","text":"Some notable applications include: Media assets: Storing images, videos, and audio files, especially when access needs are sporadic. Event logs: Preserving the continuous stream of system-generated data for analysis, though cost considerations apply for frequent data queries. Backup solutions: Providing storage for backups and disaster recovery scenarios where data is rarely accessed but must remain available for emergencies.","title":"Best use cases"},{"location":"Courses/Application%20Architecture/2.Data/999.Additional%20info/Data%20Consistency/","text":"In distributed systems, data is replicated across multiple nodes, requiring synchronization to maintain consistency. Quorum consensus ensures consistency for both read and write operations by defining specific thresholds for acknowledgments. Let\u2019s clarify some key terms: N : The total number of replicas. W : The write quorum size. A write operation is considered successful only when acknowledgments are received from at least W replicas. R : The read quorum size. A read operation is successful when responses are received from at least R replicas. Example \u00b6 In the example shown N = 3 (replicas). W = 1 does not mean data is written to only one server. Data is still replicated across all servers ( s0 , s1 , and s2 ). However, the coordinator only needs one acknowledgment to mark the write operation as successful. For instance, if s1 sends an acknowledgment, the coordinator does not need to wait for responses from s0 or s2 . The coordinator , acting as a proxy between the client and the nodes, manages these quorum checks. Tradeoff: Latency vs. Consistency \u00b6 The configuration of N , W , and R represents a tradeoff between operation speed and consistency: If W = 1 or R = 1 , operations complete quickly because the coordinator only waits for a response from one replica. If W > 1 or R > 1 , better consistency is achieved, but latency increases as the coordinator waits for responses from more replicas. Ensuring Strong Consistency \u00b6 If W + R > N , the system guarantees strong consistency. This is because there will always be at least one replica with the latest data to ensure consistency. Common Configurations \u00b6 Depending on use cases, different configurations of N , W , and R are used: Fast Read : R = 1 , W = N Data is written to all replicas, but reads return quickly by querying only one replica. Fast Write : W = 1 , R = N Writes require only one acknowledgment, but reads query all replicas for consistency. Strong Consistency : W + R > N (e.g., N = 3, W = R = 2 ) Ensures there is at least one replica with the latest data in overlap between read and write quorums. Weaker Consistency : W + R \u2264 N Strong consistency is not guaranteed. By tuning N , W , and R , systems can be optimized for desired levels of latency, throughput, and consistency based on specific requirements.","title":"Data Consistency"},{"location":"Courses/Application%20Architecture/2.Data/999.Additional%20info/Data%20Consistency/#example","text":"In the example shown N = 3 (replicas). W = 1 does not mean data is written to only one server. Data is still replicated across all servers ( s0 , s1 , and s2 ). However, the coordinator only needs one acknowledgment to mark the write operation as successful. For instance, if s1 sends an acknowledgment, the coordinator does not need to wait for responses from s0 or s2 . The coordinator , acting as a proxy between the client and the nodes, manages these quorum checks.","title":"Example"},{"location":"Courses/Application%20Architecture/2.Data/999.Additional%20info/Data%20Consistency/#tradeoff-latency-vs-consistency","text":"The configuration of N , W , and R represents a tradeoff between operation speed and consistency: If W = 1 or R = 1 , operations complete quickly because the coordinator only waits for a response from one replica. If W > 1 or R > 1 , better consistency is achieved, but latency increases as the coordinator waits for responses from more replicas.","title":"Tradeoff: Latency vs. Consistency"},{"location":"Courses/Application%20Architecture/2.Data/999.Additional%20info/Data%20Consistency/#ensuring-strong-consistency","text":"If W + R > N , the system guarantees strong consistency. This is because there will always be at least one replica with the latest data to ensure consistency.","title":"Ensuring Strong Consistency"},{"location":"Courses/Application%20Architecture/2.Data/999.Additional%20info/Data%20Consistency/#common-configurations","text":"Depending on use cases, different configurations of N , W , and R are used: Fast Read : R = 1 , W = N Data is written to all replicas, but reads return quickly by querying only one replica. Fast Write : W = 1 , R = N Writes require only one acknowledgment, but reads query all replicas for consistency. Strong Consistency : W + R > N (e.g., N = 3, W = R = 2 ) Ensures there is at least one replica with the latest data in overlap between read and write quorums. Weaker Consistency : W + R \u2264 N Strong consistency is not guaranteed. By tuning N , W , and R , systems can be optimized for desired levels of latency, throughput, and consistency based on specific requirements.","title":"Common Configurations"},{"location":"Courses/Application%20Architecture/2.Data/999.Additional%20info/MVCC%20in%20DB/","text":"Multiversion concurrency control (MVCC) provides a method for enabling simultaneous access to a database. Instead of overwriting data during updates or transactions, MVCC creates a new version or snapshot of the data. This allows only the initiating transaction to see the modified data until the changes are finalized. Meanwhile, other operations continue accessing the earlier snapshot. When older versions are no longer in use, they are automatically discarded. By retaining multiple data versions, MVCC ensures non-blocking operations, even in scenarios with heavy queries, scripts, or transactions. This approach enhances isolation and functionality compared to traditional systems and offers significantly better concurrency than two-phase locking. Key differences between MVCC and traditional locking \u00b6 Rather than locking records during updates, MVCC generates a new version with an incremented identifier. Other operations can access the previous version without contention or deadlock risks. Once updates are finalized, future reads access the updated version, and the cycle repeats for subsequent modifications. Drawbacks of MVCC \u00b6 Implementation complexity: Managing multiple versions of data requires intricate control methods. Data bloat: Maintaining multiple snapshots increases database size and can lead to inefficiencies. Advantages of forkless background saving \u00b6 One of the features enabled by MVCC is forkless background saving, which optimizes memory usage during save operations. Traditional snapshotting using the fork() system call can cause write amplification, where even small changes result in duplication of entire memory pages. Additionally, sufficient free memory must be reserved to accommodate ongoing writes, and inadequate memory can lead to failures. Forkless background saving minimizes these issues by avoiding such duplications, reducing the memory required for saving, and delivering consistent performance regardless of the size of the data being saved. This approach is particularly advantageous as database sizes and value complexities increase, making it faster and more memory-efficient.","title":"MVCC in DB"},{"location":"Courses/Application%20Architecture/2.Data/999.Additional%20info/MVCC%20in%20DB/#key-differences-between-mvcc-and-traditional-locking","text":"Rather than locking records during updates, MVCC generates a new version with an incremented identifier. Other operations can access the previous version without contention or deadlock risks. Once updates are finalized, future reads access the updated version, and the cycle repeats for subsequent modifications.","title":"Key differences between MVCC and traditional locking"},{"location":"Courses/Application%20Architecture/2.Data/999.Additional%20info/MVCC%20in%20DB/#drawbacks-of-mvcc","text":"Implementation complexity: Managing multiple versions of data requires intricate control methods. Data bloat: Maintaining multiple snapshots increases database size and can lead to inefficiencies.","title":"Drawbacks of MVCC"},{"location":"Courses/Application%20Architecture/2.Data/999.Additional%20info/MVCC%20in%20DB/#advantages-of-forkless-background-saving","text":"One of the features enabled by MVCC is forkless background saving, which optimizes memory usage during save operations. Traditional snapshotting using the fork() system call can cause write amplification, where even small changes result in duplication of entire memory pages. Additionally, sufficient free memory must be reserved to accommodate ongoing writes, and inadequate memory can lead to failures. Forkless background saving minimizes these issues by avoiding such duplications, reducing the memory required for saving, and delivering consistent performance regardless of the size of the data being saved. This approach is particularly advantageous as database sizes and value complexities increase, making it faster and more memory-efficient.","title":"Advantages of forkless background saving"},{"location":"Courses/Application%20Architecture/2.Data/999.Additional%20info/Persistent%20Data/","text":"When discussing data storage in computer systems, persistence refers to the ability of data to remain intact even after the process that created it has ended. This requires writing data to non-volatile storage. Approaches to data persistence \u00b6 Understanding persistence involves examining four primary data store designs and their relationship to persistent storage: Pure in-memory storage: These systems, like memcached or Scalaris, do not write data to non-volatile storage, sacrificing durability for speed. They are best suited for caching but not for scenarios requiring persistence. In-memory with periodic snapshots: Solutions such as Oracle Coherence or Redis periodically save in-memory data to disk. While some persistence is provided, there is a risk of data loss between snapshots. Disk-based with update-in-place writes: Systems like MySQL ISAM or MongoDB write updates directly to disk. These approaches ensure persistence but may have performance trade-offs compared to purely in-memory systems. Commitlog-based systems: Traditional OLTP databases like Oracle or SQL Server use logs to commit data changes, ensuring durability and persistence for transactional workloads. Performance vs. persistence trade-offs \u00b6 In-memory systems offer exceptional speed but are constrained by memory size and lack of durability. They are ideal for workloads with a small \"hot\" data subset but unsuitable for applications requiring full persistence. Systems leveraging disk-based or commitlog designs provide robust persistence at the cost of slower performance.","title":"Persistent Data"},{"location":"Courses/Application%20Architecture/2.Data/999.Additional%20info/Persistent%20Data/#approaches-to-data-persistence","text":"Understanding persistence involves examining four primary data store designs and their relationship to persistent storage: Pure in-memory storage: These systems, like memcached or Scalaris, do not write data to non-volatile storage, sacrificing durability for speed. They are best suited for caching but not for scenarios requiring persistence. In-memory with periodic snapshots: Solutions such as Oracle Coherence or Redis periodically save in-memory data to disk. While some persistence is provided, there is a risk of data loss between snapshots. Disk-based with update-in-place writes: Systems like MySQL ISAM or MongoDB write updates directly to disk. These approaches ensure persistence but may have performance trade-offs compared to purely in-memory systems. Commitlog-based systems: Traditional OLTP databases like Oracle or SQL Server use logs to commit data changes, ensuring durability and persistence for transactional workloads.","title":"Approaches to data persistence"},{"location":"Courses/Application%20Architecture/2.Data/999.Additional%20info/Persistent%20Data/#performance-vs-persistence-trade-offs","text":"In-memory systems offer exceptional speed but are constrained by memory size and lack of durability. They are ideal for workloads with a small \"hot\" data subset but unsuitable for applications requiring full persistence. Systems leveraging disk-based or commitlog designs provide robust persistence at the cost of slower performance.","title":"Performance vs. persistence trade-offs"},{"location":"Courses/Application%20Architecture/2.Data/999.Additional%20info/Ticket%20Server/","text":"Ticket servers are a cool way to create unique IDs. The idea is simple yet brilliant: they act as a centralized system for generating distributed primary keys. The folks at Flickr came up with this system, and it\u2019s explained in their engineering blog: Flickr's Engineering Blog Post . How does it work? \u00b6 Here\u2019s the gist: There\u2019s a single database server called the Ticket Server that uses an auto_increment feature to generate sequential IDs. These IDs are handed out to different systems that need unique identifiers. Pros: \u00b6 Straightforward numeric IDs : Simple, easy to use, and universally recognized. Simplicity : Super easy to set up, making it a good fit for small to medium-sized apps. Cons: \u00b6 Single point of failure : If the ticket server crashes, everything relying on it comes to a halt. Solution? Use multiple ticket servers. But that\u2019s not as simple as it sounds because syncing data between them can get tricky. This approach might not work for massive systems with super high demands, but for many applications, it\u2019s a great start!","title":"Ticket Server"},{"location":"Courses/Application%20Architecture/2.Data/999.Additional%20info/Ticket%20Server/#how-does-it-work","text":"Here\u2019s the gist: There\u2019s a single database server called the Ticket Server that uses an auto_increment feature to generate sequential IDs. These IDs are handed out to different systems that need unique identifiers.","title":"How does it work?"},{"location":"Courses/Application%20Architecture/2.Data/999.Additional%20info/Ticket%20Server/#pros","text":"Straightforward numeric IDs : Simple, easy to use, and universally recognized. Simplicity : Super easy to set up, making it a good fit for small to medium-sized apps.","title":"Pros:"},{"location":"Courses/Application%20Architecture/2.Data/999.Additional%20info/Ticket%20Server/#cons","text":"Single point of failure : If the ticket server crashes, everything relying on it comes to a halt. Solution? Use multiple ticket servers. But that\u2019s not as simple as it sounds because syncing data between them can get tricky. This approach might not work for massive systems with super high demands, but for many applications, it\u2019s a great start!","title":"Cons:"},{"location":"Courses/Application%20Architecture/5.Network/DNS/","text":"","title":"DNS"},{"location":"Courses/Application%20Architecture/5.Network/Firewall/","text":"","title":"Firewall"},{"location":"Courses/Application%20Architecture/5.Network/Network%20Time%20Protocol/","text":"Let\u2019s Talk About NTP (Network Time Protocol) Ever wondered how computers stay in sync with time? That\u2019s where NTP comes in. It\u2019s one of the internet\u2019s oldest tricks, created way back in 1981 by David Mills, a professor at the University of Delaware. It\u2019s super reliable, scalable, and gets the job done when it comes to time synchronization. How Does NTP Keep Time? \u00b6 Here\u2019s the gist of it: Your device (the client) sends a time request to a time server. The server sends back the time, and the client adjusts its clock accordingly. This happens a few times (like six exchanges over 10 minutes) to lock in the right time. Once that\u2019s done, your device checks in every 10 minutes or so to stay accurate. The magic happens over UDP (User Datagram Protocol) using port 123. NTP even allows multiple devices to sync up through broadcasts. Stratum Levels \u2013 The Time Pyramid \u00b6 Think of time syncing as a hierarchy, starting with the most accurate source: Stratum 0: These are the \u201ctime gods\u201d like atomic clocks or GPS systems. Stratum 1: Devices directly connected to Stratum 0 clocks. Stratum 2: Devices that get their time from Stratum 1. Stratum 3: You guessed it\u2014these sync from Stratum 2. The farther you go down the pyramid, the less accurate the time. NTPv4 \u2013 The Latest and Greatest \u00b6 The newest version, NTPv4, is super cool. It works with both IPv4 and IPv6, is backward-compatible with older versions, and is super accurate (down to a few microseconds). Plus, it has a nifty feature to find time servers automatically. How Do Devices Get the Time? \u00b6 There are two main ways devices sync up: Polling: Devices ask specific servers for the correct time. This is super accurate and reliable. Broadcasting: Servers shout out the time to everyone listening. This method is quicker but not as precise. What About SNTP? \u00b6 If full-blown NTP feels like overkill, there\u2019s SNTP (Simple Network Time Protocol). It\u2019s a lightweight version designed for less powerful devices that don\u2019t need super precise time.","title":"Network Time Protocol"},{"location":"Courses/Application%20Architecture/5.Network/Network%20Time%20Protocol/#how-does-ntp-keep-time","text":"Here\u2019s the gist of it: Your device (the client) sends a time request to a time server. The server sends back the time, and the client adjusts its clock accordingly. This happens a few times (like six exchanges over 10 minutes) to lock in the right time. Once that\u2019s done, your device checks in every 10 minutes or so to stay accurate. The magic happens over UDP (User Datagram Protocol) using port 123. NTP even allows multiple devices to sync up through broadcasts.","title":"How Does NTP Keep Time?"},{"location":"Courses/Application%20Architecture/5.Network/Network%20Time%20Protocol/#stratum-levels-the-time-pyramid","text":"Think of time syncing as a hierarchy, starting with the most accurate source: Stratum 0: These are the \u201ctime gods\u201d like atomic clocks or GPS systems. Stratum 1: Devices directly connected to Stratum 0 clocks. Stratum 2: Devices that get their time from Stratum 1. Stratum 3: You guessed it\u2014these sync from Stratum 2. The farther you go down the pyramid, the less accurate the time.","title":"Stratum Levels \u2013 The Time Pyramid"},{"location":"Courses/Application%20Architecture/5.Network/Network%20Time%20Protocol/#ntpv4-the-latest-and-greatest","text":"The newest version, NTPv4, is super cool. It works with both IPv4 and IPv6, is backward-compatible with older versions, and is super accurate (down to a few microseconds). Plus, it has a nifty feature to find time servers automatically.","title":"NTPv4 \u2013 The Latest and Greatest"},{"location":"Courses/Application%20Architecture/5.Network/Network%20Time%20Protocol/#how-do-devices-get-the-time","text":"There are two main ways devices sync up: Polling: Devices ask specific servers for the correct time. This is super accurate and reliable. Broadcasting: Servers shout out the time to everyone listening. This method is quicker but not as precise.","title":"How Do Devices Get the Time?"},{"location":"Courses/Application%20Architecture/5.Network/Network%20Time%20Protocol/#what-about-sntp","text":"If full-blown NTP feels like overkill, there\u2019s SNTP (Simple Network Time Protocol). It\u2019s a lightweight version designed for less powerful devices that don\u2019t need super precise time.","title":"What About SNTP?"},{"location":"Courses/Application%20Architecture/5.Network/OSI%20Model/","text":"","title":"OSI Model"},{"location":"Courses/Application%20Architecture/5.Network/Proxy/","text":"","title":"Proxy"},{"location":"Courses/Application%20Architecture/5.Network/Uplink/","text":"In VMware, uplink ports are physical adapter ports that connect a virtual network to a physical network. These ports are linked to physical adapters when device drivers initialize them or when virtual switch teaming policies are reconfigured. An uplink port group or dvuplink port group is created as part of a distributed switch. It serves as a template for configuring the physical connections of hosts, along with failover and load-balancing policies. Each uplink port group can contain one or more uplinks, and you map the physical NICs (network interface cards) of hosts to these uplinks on the distributed switch. At the host level, each physical NIC is assigned to an uplink port with a specific ID. Failover and load-balancing policies are applied to these uplinks, and the configurations automatically propagate to the host proxy switches (data plane). This ensures consistent failover and load-balancing settings across all physical NICs on the hosts connected to the distributed switch.","title":"Uplink"},{"location":"Courses/Application%20Architecture/5.Network/Protocols/1.Quik/","text":"","title":"1.Quik"},{"location":"Courses/Application%20Architecture/5.Network/Protocols/AMQP/","text":"","title":"AMQP"},{"location":"Courses/Application%20Architecture/5.Network/Protocols/FTP/","text":"","title":"FTP"},{"location":"Courses/Application%20Architecture/5.Network/Protocols/HTTP/","text":"","title":"HTTP"},{"location":"Courses/Application%20Architecture/5.Network/Protocols/SSH/","text":"","title":"SSH"},{"location":"Courses/Application%20Architecture/5.Network/Protocols/TCP/","text":"","title":"TCP"},{"location":"Courses/Application%20Architecture/8.Crypto/Proof%20of%20Stake/","text":"Proof of Stake (PoS) \u00b6 The Proof of Stake (PoS) mechanism is considered a more environmentally sustainable alternative to Proof of Work (PoW) networks. Unlike PoW, where miners compete to solve complex mathematical puzzles to validate transactions and earn rewards, PoS selects validators in a different manner. Validators are chosen based on the number of tokens they have staked \u2014or locked\u2014within the network. The selection process, while random, is often influenced by the quantity of tokens staked and the duration for which they are held. In a PoS system, participants lock their tokens into the network through a smart contract to serve as validators. Validators play a crucial role in maintaining the integrity of the blockchain: ensuring the network remains functional, secure, and free from manipulation. Because these tokens are locked into the system\u2014often the network\u2019s native cryptocurrency\u2014they essentially act as collateral. This process bears some resemblance to a time deposit in traditional banking, where funds are locked for a period in exchange for interest. In PoS, however, participants earn rewards, generating a return on their staked assets while contributing to the stability of the network. Delegated Proof of Stake (DPoS) \u00b6 The Delegated Proof of Stake (DPoS) model builds on the principles of PoS but introduces additional democratic measures to address issues of accessibility and fairness. In many standard PoS networks, participants with large token holdings have higher odds of being selected as validators. DPoS aims to counterbalance this by giving smaller stakeholders a voice through a voting process. In a DPoS system, all network participants hold voting rights proportional to their token holdings. Rather than directly selecting validators, participants elect representatives, often referred to as delegates or witnesses . These witnesses are tasked with validating transactions and adding new blocks to the blockchain. Delegates, on the other hand, are responsible for overseeing the network, maintaining its security, proposing changes, and guiding governance processes. This structure introduces a layer of accountability while ensuring that even smaller stakeholders can influence the network\u2019s operation. How Does Staking Work? \u00b6 To become a validator within a PoS network, participants must configure their systems, maintain up-to-date software, and ensure their setup is secure. When the network identifies a block of transactions ready for processing, it selects a validator at random using an algorithm. However, the selection process is not entirely arbitrary\u2014validators with larger staked amounts and longer staking durations typically have a higher probability of being chosen. Validators, by staking their assets, demonstrate a vested interest in the network\u2019s success. In return for their role, they receive rewards in the form of additional tokens. This system incentivizes validators to act in the network\u2019s best interest, as any malicious behavior could result in a loss of their staked funds. That said, PoS systems are not without drawbacks. A significant limitation is the steep entry barrier for many would-be validators. Participating often requires staking a substantial number of tokens, effectively limiting participation to individuals or entities with considerable holdings. As a result, networks may face issues of centralization, with larger stakeholders disproportionately influencing the validation process.","title":"Proof of Stake"},{"location":"Courses/Application%20Architecture/8.Crypto/Proof%20of%20Stake/#proof-of-stake-pos","text":"The Proof of Stake (PoS) mechanism is considered a more environmentally sustainable alternative to Proof of Work (PoW) networks. Unlike PoW, where miners compete to solve complex mathematical puzzles to validate transactions and earn rewards, PoS selects validators in a different manner. Validators are chosen based on the number of tokens they have staked \u2014or locked\u2014within the network. The selection process, while random, is often influenced by the quantity of tokens staked and the duration for which they are held. In a PoS system, participants lock their tokens into the network through a smart contract to serve as validators. Validators play a crucial role in maintaining the integrity of the blockchain: ensuring the network remains functional, secure, and free from manipulation. Because these tokens are locked into the system\u2014often the network\u2019s native cryptocurrency\u2014they essentially act as collateral. This process bears some resemblance to a time deposit in traditional banking, where funds are locked for a period in exchange for interest. In PoS, however, participants earn rewards, generating a return on their staked assets while contributing to the stability of the network.","title":"Proof of Stake (PoS)"},{"location":"Courses/Application%20Architecture/8.Crypto/Proof%20of%20Stake/#delegated-proof-of-stake-dpos","text":"The Delegated Proof of Stake (DPoS) model builds on the principles of PoS but introduces additional democratic measures to address issues of accessibility and fairness. In many standard PoS networks, participants with large token holdings have higher odds of being selected as validators. DPoS aims to counterbalance this by giving smaller stakeholders a voice through a voting process. In a DPoS system, all network participants hold voting rights proportional to their token holdings. Rather than directly selecting validators, participants elect representatives, often referred to as delegates or witnesses . These witnesses are tasked with validating transactions and adding new blocks to the blockchain. Delegates, on the other hand, are responsible for overseeing the network, maintaining its security, proposing changes, and guiding governance processes. This structure introduces a layer of accountability while ensuring that even smaller stakeholders can influence the network\u2019s operation.","title":"Delegated Proof of Stake (DPoS)"},{"location":"Courses/Application%20Architecture/8.Crypto/Proof%20of%20Stake/#how-does-staking-work","text":"To become a validator within a PoS network, participants must configure their systems, maintain up-to-date software, and ensure their setup is secure. When the network identifies a block of transactions ready for processing, it selects a validator at random using an algorithm. However, the selection process is not entirely arbitrary\u2014validators with larger staked amounts and longer staking durations typically have a higher probability of being chosen. Validators, by staking their assets, demonstrate a vested interest in the network\u2019s success. In return for their role, they receive rewards in the form of additional tokens. This system incentivizes validators to act in the network\u2019s best interest, as any malicious behavior could result in a loss of their staked funds. That said, PoS systems are not without drawbacks. A significant limitation is the steep entry barrier for many would-be validators. Participating often requires staking a substantial number of tokens, effectively limiting participation to individuals or entities with considerable holdings. As a result, networks may face issues of centralization, with larger stakeholders disproportionately influencing the validation process.","title":"How Does Staking Work?"},{"location":"Courses/Application%20Architecture/9.Security/999.Additional%20Info/1.CSRF%20Attack/","text":"A CSRF attack tricks an authenticated user into making unwanted actions on a web application without their consent. It exploits the trust that a website has in the user's browser. How CSRF Works \u00b6 Victim logs into a legitimate website (e.g., a banking site). The victim's browser stores an active session (cookie) for that site. The attacker tricks the victim into clicking a malicious link or visiting a page that sends an unauthorized request to the legitimate site. The request automatically includes the victim\u2019s session cookies , making it appear as if the request is from the victim. The server processes the request because it sees a valid session, leading to actions like changing passwords, transferring money, or deleting accounts . Scenario: Online Banking Website \u00b6 Imagine you are logged into YourBank.com and the site allows money transfers via a simple HTTP request: POST https://yourbank.com/transfer Content-Type: application/x-www-form-urlencoded amount = 1000 & to = attacker_account ` Your session is authenticated using a cookie that is automatically included in every request you make. Step-by-Step Attack Execution \u00b6 Step 1: Victim Logs into Their Bank Account \u00b6 You visit YourBank.com , enter your username and password, and successfully log in. The site sets a session cookie in your browser ( session_id=xyz123 ). Now, any request to YourBank.com is automatically authenticated using this cookie. Step 2: Attacker Prepares a Malicious Website \u00b6 The attacker creates a website ( evil.com ) with an embedded malicious request. The site includes an invisible auto-submitting form : < body > < h1 > Win a Free iPhone! Click Below: </ h1 > < form action = \"https://yourbank.com/transfer\" method = \"POST\" > < input type = \"hidden\" name = \"amount\" value = \"1000\" > < input type = \"hidden\" name = \"to\" value = \"attacker_account\" > - < input type = \"submit\" value = \"Click to Claim Prize\" > - </ form > - </ body > OR the attacker may use JavaScript to auto-submit the form when the page loads: < body onload = \"document.forms[0].submit()\" > < form action = \"https://yourbank.com/transfer\" method = \"POST\" > < input type = \"hidden\" name = \"amount\" value = \"1000\" > < input type = \"hidden\" name = \"to\" value = \"attacker_account\" > </ form > </ body > ` Step 3: Victim Clicks the Malicious Link \u00b6 You receive a phishing email or see an ad promising a free iPhone. You visit evil.com . The page automatically submits the hidden form to YourBank.com . Since your browser is still logged into YourBank.com, it automatically includes the session cookie ( session_id=xyz123 ) in the request . Step 4: The Bank Processes the Request \u00b6 YourBank.com sees a valid request with an authenticated session . The bank transfers $1,000 to the attacker's account because it believes the request came from you. You lose money without even knowing it happened. Variations of CSRF Attacks \u00b6 CSRF attacks are not limited to bank transfers. Attackers can exploit any action that requires authentication: Change Email or Password If a site allows email/password changes via a simple request: POST https://example.com/change_email Content-Type: application/x-www-form-urlencoded email = attacker@gmail.com An attacker can force a victim to change their email to attacker@gmail.com , locking them out. Delete Account GET https://example.com/delete_account - If a website **doesn't require extra verification**, a CSRF attack can delete a user\u2019s account. Purchase Items (E-Commerce Sites) POST https://shop.com/buy Content-Type: application/x-www-form-urlencoded product_id = 123 & quantity = 10 - The attacker can trick a victim into unknowingly buying expensive products.","title":"1.CSRF Attack"},{"location":"Courses/Application%20Architecture/9.Security/999.Additional%20Info/1.CSRF%20Attack/#how-csrf-works","text":"Victim logs into a legitimate website (e.g., a banking site). The victim's browser stores an active session (cookie) for that site. The attacker tricks the victim into clicking a malicious link or visiting a page that sends an unauthorized request to the legitimate site. The request automatically includes the victim\u2019s session cookies , making it appear as if the request is from the victim. The server processes the request because it sees a valid session, leading to actions like changing passwords, transferring money, or deleting accounts .","title":"How CSRF Works"},{"location":"Courses/Application%20Architecture/9.Security/999.Additional%20Info/1.CSRF%20Attack/#scenario-online-banking-website","text":"Imagine you are logged into YourBank.com and the site allows money transfers via a simple HTTP request: POST https://yourbank.com/transfer Content-Type: application/x-www-form-urlencoded amount = 1000 & to = attacker_account ` Your session is authenticated using a cookie that is automatically included in every request you make.","title":"Scenario: Online Banking Website"},{"location":"Courses/Application%20Architecture/9.Security/999.Additional%20Info/1.CSRF%20Attack/#step-by-step-attack-execution","text":"","title":"Step-by-Step Attack Execution"},{"location":"Courses/Application%20Architecture/9.Security/999.Additional%20Info/1.CSRF%20Attack/#step-1-victim-logs-into-their-bank-account","text":"You visit YourBank.com , enter your username and password, and successfully log in. The site sets a session cookie in your browser ( session_id=xyz123 ). Now, any request to YourBank.com is automatically authenticated using this cookie.","title":"Step 1: Victim Logs into Their Bank Account"},{"location":"Courses/Application%20Architecture/9.Security/999.Additional%20Info/1.CSRF%20Attack/#step-2-attacker-prepares-a-malicious-website","text":"The attacker creates a website ( evil.com ) with an embedded malicious request. The site includes an invisible auto-submitting form : < body > < h1 > Win a Free iPhone! Click Below: </ h1 > < form action = \"https://yourbank.com/transfer\" method = \"POST\" > < input type = \"hidden\" name = \"amount\" value = \"1000\" > < input type = \"hidden\" name = \"to\" value = \"attacker_account\" > - < input type = \"submit\" value = \"Click to Claim Prize\" > - </ form > - </ body > OR the attacker may use JavaScript to auto-submit the form when the page loads: < body onload = \"document.forms[0].submit()\" > < form action = \"https://yourbank.com/transfer\" method = \"POST\" > < input type = \"hidden\" name = \"amount\" value = \"1000\" > < input type = \"hidden\" name = \"to\" value = \"attacker_account\" > </ form > </ body > `","title":"Step 2: Attacker Prepares a Malicious Website"},{"location":"Courses/Application%20Architecture/9.Security/999.Additional%20Info/1.CSRF%20Attack/#step-3-victim-clicks-the-malicious-link","text":"You receive a phishing email or see an ad promising a free iPhone. You visit evil.com . The page automatically submits the hidden form to YourBank.com . Since your browser is still logged into YourBank.com, it automatically includes the session cookie ( session_id=xyz123 ) in the request .","title":"Step 3: Victim Clicks the Malicious Link"},{"location":"Courses/Application%20Architecture/9.Security/999.Additional%20Info/1.CSRF%20Attack/#step-4-the-bank-processes-the-request","text":"YourBank.com sees a valid request with an authenticated session . The bank transfers $1,000 to the attacker's account because it believes the request came from you. You lose money without even knowing it happened.","title":"Step 4: The Bank Processes the Request"},{"location":"Courses/Application%20Architecture/9.Security/999.Additional%20Info/1.CSRF%20Attack/#variations-of-csrf-attacks","text":"CSRF attacks are not limited to bank transfers. Attackers can exploit any action that requires authentication: Change Email or Password If a site allows email/password changes via a simple request: POST https://example.com/change_email Content-Type: application/x-www-form-urlencoded email = attacker@gmail.com An attacker can force a victim to change their email to attacker@gmail.com , locking them out. Delete Account GET https://example.com/delete_account - If a website **doesn't require extra verification**, a CSRF attack can delete a user\u2019s account. Purchase Items (E-Commerce Sites) POST https://shop.com/buy Content-Type: application/x-www-form-urlencoded product_id = 123 & quantity = 10 - The attacker can trick a victim into unknowingly buying expensive products.","title":"Variations of CSRF Attacks"},{"location":"Courses/Application%20Architecture/Additional%20materials/%28A%29Sync/","text":"","title":"(A)Sync"},{"location":"Courses/Application%20Architecture/Additional%20materials/ETag/","text":"An ETag (Entity Tag) is a unique identifier assigned by a web server to a specific version of a resource, typically used for caching and conditional requests. When a resource (like a webpage, image, or file) is served, the server generates an ETag and sends it in the ETag header of the response. If the client caches the resource, it also stores the ETag. When the client later requests the same resource, it can include the If-None-Match header with the ETag value. This tells the server to return the resource only if it has changed (i.e., if the ETag doesn't match). If the resource is unchanged, the server responds with a 304 Not Modified status, allowing the client to use its cached version, which saves bandwidth and reduces load times","title":"ETag"},{"location":"Courses/Application%20Architecture/Additional%20materials/JSON/","text":"","title":"JSON"},{"location":"Courses/Application%20Architecture/Additional%20materials/PCI%20DSS/","text":"The Payment Card Industry Data Security Standard (PCI DSS) is a set of security standards designed to protect card information during and after a financial transaction. What does consist of ? Well, it is a set of rules that guide companies through the storing and using personal financial data Building and maintaining secure networks firewall configuration and not using vendor-supplied defaults Protecting cardholder data encryption (of the server or storage) and secure storage practices Maintaining a vulnerability management program regularly updating antivirus software and security systems Implementing strong access control measures restricting access to cardholder data and assigning unique IDs to users Regularly monitoring and testing networks logging and tracking access to network resources Maintaining an information security policy ensuring all staff understand and follow security best practices This set of rules also differ depending on the number of transactions per year","title":"PCI DSS"},{"location":"Courses/Application%20Architecture/Additional%20materials/Raft%20protocol/","text":"","title":"Raft protocol"},{"location":"Courses/Application%20Architecture/Additional%20materials/SLA%20and%20Uptime/","text":"A Service Level Agreement (SLA) is a widely used term among service providers. It is a formal agreement between the service provider and the customer, outlining the expected level of service uptime. Major cloud providers like Amazon, Google, and Microsoft typically guarantee SLAs of 99.9% uptime or higher. Uptime is often measured in terms of \"nines\"\u2014the more nines, the greater the reliability. As shown in the table, the number of nines corresponds to the amount of downtime a system can experience. Availability % Downtime per day Downtime per week Downtime per month Downtime per year 99% 14.40 minutes 1.68 hours 7.31 hours 3.65 days 99.99% 8.64 seconds 1.01 minutes 4.38 minutes 52.60 minutes 99.999% 864.00 6.05 seconds 26.30 seconds 5.26 minutes 99.9999% 86.40 milliseconds 604.80 2.63 seconds 31.56 seconds","title":"SLA and Uptime"},{"location":"Courses/Application%20Architecture/Additional%20materials/UI/","text":"","title":"UI"},{"location":"Courses/Application%20Architecture/Additional%20materials/UUID/","text":"What is a UUID and Why Use It? \u00b6 A UUID \u2014which stands for Universally Unique Identifier \u2014is a 128-bit number used to uniquely identify information in computer systems. What makes UUIDs so appealing is their ability to produce IDs with an extremely low chance of duplication. To put that in perspective, if you were to generate 1 billion UUIDs every single second for 100 years, the odds of getting even one duplicate would only hit around 50%. In other words, the chance of \u201ccollision\u201d\u2014two identical IDs\u2014is practically zero. Here\u2019s what a UUID looks like: 09c93e62-50b4-468d-bf8a-c07e1040bfb2 UUIDs can be generated by individual machines (or servers) completely independently, without needing to check in with each other. That\u2019s a huge advantage. How Does It Work? \u00b6 In systems using UUIDs, every web server has its own ID generator. These servers create their own IDs whenever they need to, without relying on any central coordinator. That means there\u2019s no risk of servers \u201cstepping on each other\u2019s toes\u201d when generating new IDs. Here\u2019s the basic idea: Server A generates its IDs. Server B generates its IDs. Neither one has to ask permission or wait for the other\u2014both just do their own thing. It\u2019s clean, simple, and fast . The Upsides \u00b6 Simplicity : Generating UUIDs is dead easy. Since servers don\u2019t need to talk to each other, there\u2019s no messy synchronization or waiting around. Scalability : Adding more servers? No problem. Each one just generates its own IDs, so the system scales naturally as you grow. The Downsides \u00b6 Of course, UUIDs aren\u2019t perfect. Here are a few trade-offs: They\u2019re too long : UUIDs are 128 bits, but sometimes you only need a smaller 64-bit ID. They\u2019re random : UUIDs don\u2019t increase with time. If you need IDs that follow an order (like 1, 2, 3...), UUIDs won\u2019t work well. Not always numeric : If you\u2019re looking for simple numbers as IDs, UUIDs might disappoint you\u2014they come with letters and dashes, which can be less clean for some systems. In short, UUIDs are great for systems that need unique IDs without any fuss. They\u2019re easy to use, and servers can generate them independently, which keeps things fast and scalable. The only downside? They might be overkill for simpler requirements.","title":"UUID"},{"location":"Courses/Application%20Architecture/Additional%20materials/UUID/#what-is-a-uuid-and-why-use-it","text":"A UUID \u2014which stands for Universally Unique Identifier \u2014is a 128-bit number used to uniquely identify information in computer systems. What makes UUIDs so appealing is their ability to produce IDs with an extremely low chance of duplication. To put that in perspective, if you were to generate 1 billion UUIDs every single second for 100 years, the odds of getting even one duplicate would only hit around 50%. In other words, the chance of \u201ccollision\u201d\u2014two identical IDs\u2014is practically zero. Here\u2019s what a UUID looks like: 09c93e62-50b4-468d-bf8a-c07e1040bfb2 UUIDs can be generated by individual machines (or servers) completely independently, without needing to check in with each other. That\u2019s a huge advantage.","title":"What is a UUID and Why Use It?"},{"location":"Courses/Application%20Architecture/Additional%20materials/UUID/#how-does-it-work","text":"In systems using UUIDs, every web server has its own ID generator. These servers create their own IDs whenever they need to, without relying on any central coordinator. That means there\u2019s no risk of servers \u201cstepping on each other\u2019s toes\u201d when generating new IDs. Here\u2019s the basic idea: Server A generates its IDs. Server B generates its IDs. Neither one has to ask permission or wait for the other\u2014both just do their own thing. It\u2019s clean, simple, and fast .","title":"How Does It Work?"},{"location":"Courses/Application%20Architecture/Additional%20materials/UUID/#the-upsides","text":"Simplicity : Generating UUIDs is dead easy. Since servers don\u2019t need to talk to each other, there\u2019s no messy synchronization or waiting around. Scalability : Adding more servers? No problem. Each one just generates its own IDs, so the system scales naturally as you grow.","title":"The Upsides"},{"location":"Courses/Application%20Architecture/Additional%20materials/UUID/#the-downsides","text":"Of course, UUIDs aren\u2019t perfect. Here are a few trade-offs: They\u2019re too long : UUIDs are 128 bits, but sometimes you only need a smaller 64-bit ID. They\u2019re random : UUIDs don\u2019t increase with time. If you need IDs that follow an order (like 1, 2, 3...), UUIDs won\u2019t work well. Not always numeric : If you\u2019re looking for simple numbers as IDs, UUIDs might disappoint you\u2014they come with letters and dashes, which can be less clean for some systems. In short, UUIDs are great for systems that need unique IDs without any fuss. They\u2019re easy to use, and servers can generate them independently, which keeps things fast and scalable. The only downside? They might be overkill for simpler requirements.","title":"The Downsides"},{"location":"Courses/Application%20Architecture/Cases/Cases/","text":"","title":"Cases"},{"location":"Courses/Application%20Architecture/Excalidraw/Monolith.excalidraw/","text":"\u26a0 Switch to EXCALIDRAW VIEW in the MORE OPTIONS menu of this document. \u26a0 You can decompress Drawing data with the command palette: 'Decompress current Excalidraw file'. For more info check in plugin settings under 'Saving' Excalidraw Data \u00b6 Text Elements \u00b6 Business Logic ^1QRHB8I5 Background Processes ^Eon269LW Data Interfaces ^0SqifRjG User inerface ^trbkeBCl And many more ^iUxlXOrw Single application ^kFrL3V5m %% Drawing \u00b6 N4KAkARALgngDgUwgLgAQQQDwMYEMA2AlgCYBOuA7hADTgQBuCpAzoQPYB2KqATLZMzYBXUtiRoIACyhQ4zZAHoFAc0JRJQgEYA6bGwC2CgF7N6hbEcK4OCtptbErHALRY8RMpWdx8Q1TdIEfARcZgRmBShcZQUebQBmbQAGGjoghH0EDihmbgBtcDBQMBLoeHF0QOwojmVg1JLIRhZ2LjR4gDYk/lLm1k4AOU4xbgBGePiAdgAOaaT4pIBWHshC DmIsbghcFJWIQmYAEXSoBGJuADMCML2SLYBZAC1JAAVFgFUAGQB9Z3iAKQAYhRRqMABK4OAAUSgACUGqULoR8PgAMqweoSQQeBECKCkNgAawQAHUSOpuHxCniCcT0TBMehsXc9gS/JIOOFcmhRns2HBcNg1DAxklutTtrVGeLGhBMNxnKNFpNkvFRjxlhKRWg/kkVdN4osAJxTTWy5j4okIADCbHwbFIWwAxKMEK7XbiIJpBYTlGz1rb7Y6JPjrM wBYFsp6KOTJJSACwdbTTI3xxZqjV7SQIQjKaTcePTFULPXTdVm0phM7cDozRY8eOTI17P3COAASWI3NQeQAunsLuRMp3uBwhCjWcJ1pzmN3irLYIhuPFqQBfPaaKfEKHBTLZbt9vZCODEXCnc48yY8I0dI2TDqjPWTeJ7IgcQmj8f4V9sbDEi+oFc+BhIU66FPOkCLhUEBVDUdRIHsfStGM8RUrKSGDMMFSjCaSRpos0wVqs6ybBIuDxJ6BzHME5 6XNcCC3ABEAAPKEKiFxGvoRi+AAKmChCYNazgkt8ADi7YAFYAIqekiKL0oyEDMucewWrSpKxpSqmWnSGLQcpnpsnmM7dryEr8oKwqijKpTWPB3A2ZA8o6qM8ZxPeoyTKMRZERA2qoIqd7JrW3mTL5alWoGDrOu6boIRK3p/q2QgBna0UhuQHDhrgkZQNGmloDwOHaEVSQmmFWY5nmeVoMq2gdPGSQdJmEpVgBqEdNMPA8DML4SslHZdvk/YSoOuD DgBY4ThK/rECZ3AQZB5TLmuG5bjuGRZDk+TUkUu1LUuIZYHlKylHcEijFJsJggAQtM7aLBAu2rtSI2ysep60Ze163mFRp4Wqr5rB+aBTd+Er2n+1ZoEBIElGBJQQWUh3oKcmAnRKGFtLwZnoUw/QcEMHAjDyt6po+ha42dJHOeguDxpRRwnNDgH0YxWyXddd0PbJyJonpWwGdp6lksQFKFcLVoKfpdosjNwjGVyYx8gKQqwNZex2dKey04qeHJB0 D6TM+nWjA1TV7P5ipltoiyJj5kvElFwboC6cUehuPrJalQZbKGWURlt+Vi3GPLxqMyY4YRjadKhZZoaU2a5vmhXhQgLPple/0LH1soDZ2B5vYiQ4ICOoNfpOKVzUr5fTbKm5Vxte7bWgh4Sh9Z4s55P13saANU5Ab4g6gYM/lDAGwwxo2cFAqKEEYFQ8I5EAXDPgLjci/m+WjNXoDdQisCZqCfGwqjYIZlA8cdWz74fXLH6f5iejvACCRDKNjEBi NkTCes0UDmAIG/XMn99AkGIPUPYehsi4DWEwMu6BWLsU4txIQfEBJCREuJaSnoHS5jWAQK+6Mb4HzgbOB+Z9PS4CEFANgsJwjzwqPiIQU9ZRvgQGCKqKdUARw1KBHoSMoJbFglreKeMWicEpEsRC+NWhExJjwwiJY5jxmbBKNYGxabbEercJmNEWaT3ZhIaYFwJIv3iPoZiUk2KPHoHAQ41pATPhfosAAGrzeSAssSyxUq1HSGkQ5aT8epaWgsfG GQVhyGuPCVaWXVjyMUmspQVGXrrVy7l5jlktgqUEix6pGiNEVB2wTIppRdhAN2sVPSJV9LNZ2ftMrZVysHcWOMjQlUfOVXySdqo1mXm1MYyoeAGnrEaXy+chqtyLpAMaE1Px11KLNeaaBFrIwqCuRoCNSgN3WE3LaB5dqLQOtBHeuIzpMShJwHgt5Pgkieo0MAL1GjTIgB3L6PCs6/UWHhZqudShD3meDNhv5/x0WAggfh4EJRCKOsQmREjsaPl8 ljeR2FExeW+fGVRtwaZbFwB0Rm1EEDvMMeoi5Vybl3IHHzUJ3icSOwCa0hONIpZeKZOEyuitZzK3MqrKyCTl6iIcjrBUKYSp3i6JMNyoVfJW1BEkfJhSZUMvqRISpcVqlezqWUhpYZA5Rj2DGQJPJ1QJAKf9Hg8R4ymkqsnXe3zVLpwAuTDonRJUtjZINQuA4S4INHvLKuyyR4VwSutXc+zhpHhPJ3AC3cby9yaksSYQN3yArHqCmGbNp7ZDngvK RA414b3wFvPYpyJA3S1VuAAOhwF4BIxCznCBfCgRDd4QArUlWaqA62/i5E20tx1gEfy2N/U4jp4UAPcEO0B4DIESmgVEOBpAEEQBMWYixVibF2IcU4+ILj3F8lIPgjghDr7lsrVXbt9a+25E1jQuhDC81oGYaw/5cDOF2rGCVRYkLEbQuWhIERyS/6yMke0PC8KCaospOqK1oITS+Q0aROmkxCXMwnlm2U510ADEkKiQ42A4AcEBJwR4FaJJGmIP 8a0hAOiAg8fzBkMt6UlOJKLJlDLaXspY7KIyUTuU8liWrfyj5BXAbQKknJ4cEioUyS1WUVsJi22auFfxqrXaxQ9glC9Pt0qo0afqjGsojVMuKqVLptreloC6I6ruSRrmzA6PWZN/VPUF0jaNX1k0Q28a3EG1ZMLUAbPhmtRu4b9w7QeUctZftr6nVWExJIqIACOhALiwgkqJe5mzXpRs+l3T5vcl5Lz+YPYGaaIYgoMfRX9e0FwAdRnFzGoHsbdW ZQwFr0GTUNkWM1UEA99i4rItMND+iMPgqMegJLqX0uZYY1xpSHLWOMtDrwTjbLFs8cWZEoNA2LLCY1hKIVEmRUuU8ske8qjLVlWVJk7JLkkxNXk5WNTOq1WabEdsnTxB1PQAMzlIOhqCofIVWTQiN4cKuSe5Z7hz2BBOrGIRV1PXQQerbO5qZPrxql28wsyASzon+vrmGzaEXMft2je8uNXzGpzFKxAAFtcgX/Kq+Nm42bZ6MPzRz9eYDi3cG3me 9Ahwzy4FQO2H+pArgNvQKyS+QuIAi6iOLyX0vwiy+hYO9+n9R2/wnYA/A06thgOIBAz7kAF2wM5MupiuH8OEeI6R8jlHqO0fo4e49p7iESCV2LiXY61fMA17KahtD6GsCfagF9KaOFcN3rwn9IWoX1ZRjBBA1QhWQeQoVeYWfMLE2whqRMt41QDaQ1o3AhxRvEuqxNslWxDhTFRO8WEepvhuJgFAb4HQBhGnuMQT4uxRo0o20LZb7HVvtYirpJjY Stv4529EvbfL4k8MSUd8TqBJMuSxbbUEXlikKYVNcpM8RUzpiyct376qtP12+79/2TTAcShM5PsznSbUSh6dwuq5sVO2YAiXiNGmC6EWG+XpwmW9U82xz9R8220DWiQCwa2C0eVC12XCxbh7EOX2hi1hSMweX2CYnxE0GJBumtHwGy3hlywp3y1jUKz+jtiSGmBTWHiJxZ3HjBThlQOT1KEC2gCa3EQJlFGYOawRS614DKgKQfHGBcywyGzpihGr xJUw3OQaRIIQDIIoOpU8VnzpTlnNH8QnyCQMJCVHyW183ZF2yE35TXzE3shOwlF1gKWTAhyLDCgbEImGXuwCg1AVT1GlUPxe3Umvw+01U7S3Af3+2aSB2NR4WvG0DvGALCkxTvCWG6Tj0pDTi7niBAKlVTD1DR2PAxx7BeVmRxwqwsOnEJzgMgB2W3AwKgPekpwKx7nvDFByPa0Z2DTxwZ1Z04NfRmRnlzUXmXlXmyF503gFwHW93QHeDCFIFQCX TV2bVbS2DmKYEWOt2WOmKgCNwkF13HVEMnSAW12N1nXNy/hnit3gSYkb0mGb1b0mHb1cU72717370H1wSPX8C9zbXWIWKWMFAuNDwfQjyYVIBYRjw/Ssx4W/Vq0EQazTwz2Azz2xjg1RPEOkMmCSCVHTBxU0TxXhF0SJWULrywyYh4AAGlnBJAYAeIOAOgpJmJ3gAA1DgCST4ZLdsG6Fkwkf4ebMw+fJSQw4HKffxBbMfCwrlUyaw1fUTJJewrfU 7AKcYaYbQe8MZDMWVZcNUw0O2TqCqK/N7DTd2C4mpb2H7Y0v7PVAHA1F/UU9/MqT/WUb/XeGzVqBHMOMUJzVyZ8Qor1DzWUMo2AnognATVAJAlGFArZWoknZuA5KLHAvg05eLQgrYQgd4TAfAVxZiUgKgZ6agpo2gsYegroCYO2Fgio9gjNVmcFeE/9VPFM0QoQwqP05suRLCBMdUDFW8fE5DbYVEJQ2vdnck9MzM7M3MqgbQxjRSSUoIq0IwiWZ bCU8w+A6UnlWUfbGw+UjfRU7fFUsUbQCmHEo0M2cA/6DobwsA5MMA+2Q0kw0pX2d7U0sI2pCIq0x/QzFpVbVyCOIsApcYCYMUGYdUGHXeOHJST01AfItMLyKYf04otuIMrzKshfBA8Mtg2MsLUnTApC0oN5Fo+NNossVCSspndNYcgYleIYrnHPAtcYotEtTXGYiAF+dYVAfQawGADih0C48gFtBXNi4gDiriniwIZ+LXEBEdLaPXI4g3PY9AE3M 3T0S3JdFdKkmkukhkpk1k9kzk7k3k/kj3H4/AVYiQISkSjgbi/QXiqhe9cPWiqPCEqi9haE7hBPeslPE5AQ3oFrSkB8DEzsnkBYNMA0oqPsivC4IctnKi7DCAQkQEUgT4eIFkxYfQAU3Q7jfQ+ctjUU9bTKzbbKtC9cwTXlOJETdfEPTffcxUbqZMM/aYa1e80oRTRqeqMKECiC6fG0K0m/M0+/D8qI5/YzYHTyNUjq+OdIz9VOAAsYf6JYHI/8h CyZEorHOZcigNKojCmor0OMiNcnIsmNEs1opzRqnIsi7o5nQePozNMkxEGiyPJeeiqACY/nWqHYrYOeZJVASEHwQBABSROXASlir6+CH6uAP6vAAGrgHYhShnBAC4fApoJgY4w3U4kMfkFSq4tS3HK6iAPBEysy9AUG4IcGyGs8ZCO9MPR9CoUlNhd9DInkOEqgsAN6bYCG9ETuBaQoaAbMTILYd+fMHoBgQgBACgDtN8qua/C4GW2WhEL+EQXKd sU4fQdEYI3q0I4W7ARWraZWjICWi0yIm06Inm7W0gJWlWwEEfAquchW823WlWtWhcvK02nW7IPW1W8UwU7Ku2i2jIWERfcMqmX2h2jIZiFfCqmyEO92y2wtPnJi0oM2v2/QEjHNRyp612+2mOjIVtOG4IRG+WpO0Oz2nKXY82tgCgbMXAXGrWt2qAD2qEdYF+cuyukIJiHKAkKcnm5gbAAkFEdxQqS1YKeNEAyVdJEQysXuu0fAAATWXAIiPKvGt RzmVFcj+QgCMDYAMG5vQgIBYS/TClq2jvrpVoDvQu7Bgi3Hlr9BIGGO51KBvstKfIjIlBujtCYidGtCNC/q/s9HoWUHHBymdChEOBAZAeyy2WPqduJHDuhu7DYIgDgECDMGEGYFEiPWIDvo2uLmx3oRIiPVqB3tKCyFwE0GCAAmj3nSIDgG4EodlBPX5ufWcr5BoXYVoecqPrsAknTxyFRBPTgHuDYA2EbtIfIf6PAARhXmRCbRWRelXCAA= %%","title":"Monolith.excalidraw"},{"location":"Courses/Application%20Architecture/Excalidraw/Monolith.excalidraw/#excalidraw-data","text":"","title":"Excalidraw Data"},{"location":"Courses/Application%20Architecture/Excalidraw/Monolith.excalidraw/#text-elements","text":"Business Logic ^1QRHB8I5 Background Processes ^Eon269LW Data Interfaces ^0SqifRjG User inerface ^trbkeBCl And many more ^iUxlXOrw Single application ^kFrL3V5m %%","title":"Text Elements"},{"location":"Courses/Application%20Architecture/Excalidraw/Monolith.excalidraw/#drawing","text":"N4KAkARALgngDgUwgLgAQQQDwMYEMA2AlgCYBOuA7hADTgQBuCpAzoQPYB2KqATLZMzYBXUtiRoIACyhQ4zZAHoFAc0JRJQgEYA6bGwC2CgF7N6hbEcK4OCtptbErHALRY8RMpWdx8Q1TdIEfARcZgRmBShcZQUebQBmbQAGGjoghH0EDihmbgBtcDBQMBLoeHF0QOwojmVg1JLIRhZ2LjR4gDYk/lLm1k4AOU4xbgBGePiAdgAOaaT4pIBWHshC DmIsbghcFJWIQmYAEXSoBGJuADMCML2SLYBZAC1JAAVFgFUAGQB9Z3iAKQAYhRRqMABK4OAAUSgACUGqULoR8PgAMqweoSQQeBECKCkNgAawQAHUSOpuHxCniCcT0TBMehsXc9gS/JIOOFcmhRns2HBcNg1DAxklutTtrVGeLGhBMNxnKNFpNkvFRjxlhKRWg/kkVdN4osAJxTTWy5j4okIADCbHwbFIWwAxKMEK7XbiIJpBYTlGz1rb7Y6JPjrM wBYFsp6KOTJJSACwdbTTI3xxZqjV7SQIQjKaTcePTFULPXTdVm0phM7cDozRY8eOTI17P3COAASWI3NQeQAunsLuRMp3uBwhCjWcJ1pzmN3irLYIhuPFqQBfPaaKfEKHBTLZbt9vZCODEXCnc48yY8I0dI2TDqjPWTeJ7IgcQmj8f4V9sbDEi+oFc+BhIU66FPOkCLhUEBVDUdRIHsfStGM8RUrKSGDMMFSjCaSRpos0wVqs6ybBIuDxJ6BzHME5 6XNcCC3ABEAAPKEKiFxGvoRi+AAKmChCYNazgkt8ADi7YAFYAIqekiKL0oyEDMucewWrSpKxpSqmWnSGLQcpnpsnmM7dryEr8oKwqijKpTWPB3A2ZA8o6qM8ZxPeoyTKMRZERA2qoIqd7JrW3mTL5alWoGDrOu6boIRK3p/q2QgBna0UhuQHDhrgkZQNGmloDwOHaEVSQmmFWY5nmeVoMq2gdPGSQdJmEpVgBqEdNMPA8DML4SslHZdvk/YSoOuD DgBY4ThK/rECZ3AQZB5TLmuG5bjuGRZDk+TUkUu1LUuIZYHlKylHcEijFJsJggAQtM7aLBAu2rtSI2ysep60Ze163mFRp4Wqr5rB+aBTd+Er2n+1ZoEBIElGBJQQWUh3oKcmAnRKGFtLwZnoUw/QcEMHAjDyt6po+ha42dJHOeguDxpRRwnNDgH0YxWyXddd0PbJyJonpWwGdp6lksQFKFcLVoKfpdosjNwjGVyYx8gKQqwNZex2dKey04qeHJB0 D6TM+nWjA1TV7P5ipltoiyJj5kvElFwboC6cUehuPrJalQZbKGWURlt+Vi3GPLxqMyY4YRjadKhZZoaU2a5vmhXhQgLPple/0LH1soDZ2B5vYiQ4ICOoNfpOKVzUr5fTbKm5Vxte7bWgh4Sh9Z4s55P13saANU5Ab4g6gYM/lDAGwwxo2cFAqKEEYFQ8I5EAXDPgLjci/m+WjNXoDdQisCZqCfGwqjYIZlA8cdWz74fXLH6f5iejvACCRDKNjEBi NkTCes0UDmAIG/XMn99AkGIPUPYehsi4DWEwMu6BWLsU4txIQfEBJCREuJaSnoHS5jWAQK+6Mb4HzgbOB+Z9PS4CEFANgsJwjzwqPiIQU9ZRvgQGCKqKdUARw1KBHoSMoJbFglreKeMWicEpEsRC+NWhExJjwwiJY5jxmbBKNYGxabbEercJmNEWaT3ZhIaYFwJIv3iPoZiUk2KPHoHAQ41pATPhfosAAGrzeSAssSyxUq1HSGkQ5aT8epaWgsfG GQVhyGuPCVaWXVjyMUmspQVGXrrVy7l5jlktgqUEix6pGiNEVB2wTIppRdhAN2sVPSJV9LNZ2ftMrZVysHcWOMjQlUfOVXySdqo1mXm1MYyoeAGnrEaXy+chqtyLpAMaE1Px11KLNeaaBFrIwqCuRoCNSgN3WE3LaB5dqLQOtBHeuIzpMShJwHgt5Pgkieo0MAL1GjTIgB3L6PCs6/UWHhZqudShD3meDNhv5/x0WAggfh4EJRCKOsQmREjsaPl8 ljeR2FExeW+fGVRtwaZbFwB0Rm1EEDvMMeoi5Vybl3IHHzUJ3icSOwCa0hONIpZeKZOEyuitZzK3MqrKyCTl6iIcjrBUKYSp3i6JMNyoVfJW1BEkfJhSZUMvqRISpcVqlezqWUhpYZA5Rj2DGQJPJ1QJAKf9Hg8R4ymkqsnXe3zVLpwAuTDonRJUtjZINQuA4S4INHvLKuyyR4VwSutXc+zhpHhPJ3AC3cby9yaksSYQN3yArHqCmGbNp7ZDngvK RA414b3wFvPYpyJA3S1VuAAOhwF4BIxCznCBfCgRDd4QArUlWaqA62/i5E20tx1gEfy2N/U4jp4UAPcEO0B4DIESmgVEOBpAEEQBMWYixVibF2IcU4+ILj3F8lIPgjghDr7lsrVXbt9a+25E1jQuhDC81oGYaw/5cDOF2rGCVRYkLEbQuWhIERyS/6yMke0PC8KCaospOqK1oITS+Q0aROmkxCXMwnlm2U510ADEkKiQ42A4AcEBJwR4FaJJGmIP 8a0hAOiAg8fzBkMt6UlOJKLJlDLaXspY7KIyUTuU8liWrfyj5BXAbQKknJ4cEioUyS1WUVsJi22auFfxqrXaxQ9glC9Pt0qo0afqjGsojVMuKqVLptreloC6I6ruSRrmzA6PWZN/VPUF0jaNX1k0Q28a3EG1ZMLUAbPhmtRu4b9w7QeUctZftr6nVWExJIqIACOhALiwgkqJe5mzXpRs+l3T5vcl5Lz+YPYGaaIYgoMfRX9e0FwAdRnFzGoHsbdW ZQwFr0GTUNkWM1UEA99i4rItMND+iMPgqMegJLqX0uZYY1xpSHLWOMtDrwTjbLFs8cWZEoNA2LLCY1hKIVEmRUuU8ske8qjLVlWVJk7JLkkxNXk5WNTOq1WabEdsnTxB1PQAMzlIOhqCofIVWTQiN4cKuSe5Z7hz2BBOrGIRV1PXQQerbO5qZPrxql28wsyASzon+vrmGzaEXMft2je8uNXzGpzFKxAAFtcgX/Kq+Nm42bZ6MPzRz9eYDi3cG3me 9Ahwzy4FQO2H+pArgNvQKyS+QuIAi6iOLyX0vwiy+hYO9+n9R2/wnYA/A06thgOIBAz7kAF2wM5MupiuH8OEeI6R8jlHqO0fo4e49p7iESCV2LiXY61fMA17KahtD6GsCfagF9KaOFcN3rwn9IWoX1ZRjBBA1QhWQeQoVeYWfMLE2whqRMt41QDaQ1o3AhxRvEuqxNslWxDhTFRO8WEepvhuJgFAb4HQBhGnuMQT4uxRo0o20LZb7HVvtYirpJjY Stv4529EvbfL4k8MSUd8TqBJMuSxbbUEXlikKYVNcpM8RUzpiyct376qtP12+79/2TTAcShM5PsznSbUSh6dwuq5sVO2YAiXiNGmC6EWG+XpwmW9U82xz9R8220DWiQCwa2C0eVC12XCxbh7EOX2hi1hSMweX2CYnxE0GJBumtHwGy3hlywp3y1jUKz+jtiSGmBTWHiJxZ3HjBThlQOT1KEC2gCa3EQJlFGYOawRS614DKgKQfHGBcywyGzpihGr xJUw3OQaRIIQDIIoOpU8VnzpTlnNH8QnyCQMJCVHyW183ZF2yE35TXzE3shOwlF1gKWTAhyLDCgbEImGXuwCg1AVT1GlUPxe3Umvw+01U7S3Af3+2aSB2NR4WvG0DvGALCkxTvCWG6Tj0pDTi7niBAKlVTD1DR2PAxx7BeVmRxwqwsOnEJzgMgB2W3AwKgPekpwKx7nvDFByPa0Z2DTxwZ1Z04NfRmRnlzUXmXlXmyF503gFwHW93QHeDCFIFQCX TV2bVbS2DmKYEWOt2WOmKgCNwkF13HVEMnSAW12N1nXNy/hnit3gSYkb0mGb1b0mHb1cU72717370H1wSPX8C9zbXWIWKWMFAuNDwfQjyYVIBYRjw/Ssx4W/Vq0EQazTwz2Azz2xjg1RPEOkMmCSCVHTBxU0TxXhF0SJWULrywyYh4AAGlnBJAYAeIOAOgpJmJ3gAA1DgCST4ZLdsG6Fkwkf4ebMw+fJSQw4HKffxBbMfCwrlUyaw1fUTJJewrfU 7AKcYaYbQe8MZDMWVZcNUw0O2TqCqK/N7DTd2C4mpb2H7Y0v7PVAHA1F/UU9/MqT/WUb/XeGzVqBHMOMUJzVyZ8Qor1DzWUMo2AnognATVAJAlGFArZWoknZuA5KLHAvg05eLQgrYQgd4TAfAVxZiUgKgZ6agpo2gsYegroCYO2Fgio9gjNVmcFeE/9VPFM0QoQwqP05suRLCBMdUDFW8fE5DbYVEJQ2vdnck9MzM7M3MqgbQxjRSSUoIq0IwiWZ bCU8w+A6UnlWUfbGw+UjfRU7fFUsUbQCmHEo0M2cA/6DobwsA5MMA+2Q0kw0pX2d7U0sI2pCIq0x/QzFpVbVyCOIsApcYCYMUGYdUGHXeOHJST01AfItMLyKYf04otuIMrzKshfBA8Mtg2MsLUnTApC0oN5Fo+NNossVCSspndNYcgYleIYrnHPAtcYotEtTXGYiAF+dYVAfQawGADih0C48gFtBXNi4gDiriniwIZ+LXEBEdLaPXI4g3PY9AE3M 3T0S3JdFdKkmkukhkpk1k9kzk7k3k/kj3H4/AVYiQISkSjgbi/QXiqhe9cPWiqPCEqi9haE7hBPeslPE5AQ3oFrSkB8DEzsnkBYNMA0oqPsivC4IctnKi7DCAQkQEUgT4eIFkxYfQAU3Q7jfQ+ctjUU9bTKzbbKtC9cwTXlOJETdfEPTffcxUbqZMM/aYa1e80oRTRqeqMKECiC6fG0K0m/M0+/D8qI5/YzYHTyNUjq+OdIz9VOAAsYf6JYHI/8h CyZEorHOZcigNKojCmor0OMiNcnIsmNEs1opzRqnIsi7o5nQePozNMkxEGiyPJeeiqACY/nWqHYrYOeZJVASEHwQBABSROXASlir6+CH6uAP6vAAGrgHYhShnBAC4fApoJgY4w3U4kMfkFSq4tS3HK6iAPBEysy9AUG4IcGyGs8ZCO9MPR9CoUlNhd9DInkOEqgsAN6bYCG9ETuBaQoaAbMTILYd+fMHoBgQgBACgDtN8qua/C4GW2WhEL+EQXKd sU4fQdEYI3q0I4W7ARWraZWjICWi0yIm06Inm7W0gJWlWwEEfAquchW823WlWtWhcvK02nW7IPW1W8UwU7Ku2i2jIWERfcMqmX2h2jIZiFfCqmyEO92y2wtPnJi0oM2v2/QEjHNRyp612+2mOjIVtOG4IRG+WpO0Oz2nKXY82tgCgbMXAXGrWt2qAD2qEdYF+cuyukIJiHKAkKcnm5gbAAkFEdxQqS1YKeNEAyVdJEQysXuu0fAAATWXAIiPKvGt RzmVFcj+QgCMDYAMG5vQgIBYS/TClq2jvrpVoDvQu7Bgi3Hlr9BIGGO51KBvstKfIjIlBujtCYidGtCNC/q/s9HoWUHHBymdChEOBAZAeyy2WPqduJHDuhu7DYIgDgECDMGEGYFEiPWIDvo2uLmx3oRIiPVqB3tKCyFwE0GCAAmj3nSIDgG4EodlBPX5ufWcr5BoXYVoecqPrsAknTxyFRBPTgHuDYA2EbtIfIf6PAARhXmRCbRWRelXCAA= %%","title":"Drawing"}]}