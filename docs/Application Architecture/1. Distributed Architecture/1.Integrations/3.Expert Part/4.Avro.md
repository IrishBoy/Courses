Apache Avro is a data serialization framework 
Basically, it is another way of serializing [[JSON]]

Key features of Avro include:

1. **Compact Serialization**: Avro uses a compact binary format f
2. **Schema-Based**: Avro relies on schemas to define the structure of the data being serialized. The schema is stored alongside the data, allowing for schema evolution. This means you can change the schema over time (e.g., adding or removing fields) without breaking backward or forward compatibility.
4. **Interoperability**: Since the schema is embedded with the data, any system that reads Avro-serialized data can interpret it without needing additional information, making it great for distributed systems.
### Typical Use Cases:

- **Data Storage**: Avro is often used with systems like Hadoop, Apache Kafka, and Apache Flink for storing large amounts of data efficiently.
- **Data Interchange**: Since Avro is language-neutral, it is widely used for cross-language RPC (Remote Procedure Calls) and message passing in distributed systems.

It is commonly paired with other technologies like [[5.kafka]] for message queuing, or [[Hadoop]] for big data storage and processing.

## How does it work?

### 1. **Basic Avro Schema and Serialization (Python)**

First, you define the Avro schema in JSON format. This schema will describe the structure of the data you're going to serialize.

#### Avro Schema (JSON)
```json
{
  "type": "record",
  "name": "User",
  "fields": [
    {"name": "name", "type": "string"},
    {"name": "age", "type": "int"},
    {"name": "email", "type": ["null", "string"], "default": null}
  ]
}
```
This schema describes a `User` record with three fields: `name` (string), `age` (integer), and `email` (nullable string with a default value of `null`).

#### Python Example (Serialization and Deserialization)
Now, let's use Python to serialize and deserialize data based on this Avro schema.

1. Install the `avro` package if you haven't already:
   ```bash
   pip install avro-python3
   ```

2. Example Python script:

```python
import avro.schema
import avro.io
import io


schema_json = """
{
  "type": "record",
  "name": "User",
  "fields": [
    {"name": "name", "type": "string"},
    {"name": "age", "type": "int"},
    {"name": "email", "type": ["null", "string"], "default": null}
  ]
}
"""


schema = avro.schema.parse(schema_json)

# Create an example user record
user = {
    "name": "John Doe",
    "age": 28,
    "email": "john.doe@example.com"
}


bytes_writer = io.BytesIO()
encoder = avro.io.BinaryEncoder(bytes_writer)
datum_writer = avro.io.DatumWriter(schema)
datum_writer.write(user, encoder)
serialized_data = bytes_writer.getvalue()

print(f"Serialized data: {serialized_data}")


bytes_reader = io.BytesIO(serialized_data)
decoder = avro.io.BinaryDecoder(bytes_reader)
datum_reader = avro.io.DatumReader(schema)
deserialized_user = datum_reader.read(decoder)

print(f"Deserialized data: {deserialized_user}")
```

#### Output:
```plaintext
Serialized data: b'\x06John Doe8\x1ejohn.doe@example.com'
Deserialized data: {'name': 'John Doe', 'age': 28, 'email': 'john.doe@example.com'}
```

### 2. **Avro with Nullable Fields**

Avro supports nullable fields using a **union type**. Here's an example where the `email` field can either be a string or `null`.

#### Schema with Nullable Field (JSON)
```json
{
  "type": "record",
  "name": "User",
  "fields": [
    {"name": "name", "type": "string"},
    {"name": "age", "type": "int"},
    {"name": "email", "type": ["null", "string"], "default": null}
  ]
}
```

#### Python Example (Handling Null Values)
```python
user_without_email = {
    "name": "Jane Doe",
    "age": 25,
    "email": None
}


bytes_writer = io.BytesIO()
encoder = avro.io.BinaryEncoder(bytes_writer)
datum_writer = avro.io.DatumWriter(schema)
datum_writer.write(user_without_email, encoder)
serialized_data = bytes_writer.getvalue()

print(f"Serialized data (without email): {serialized_data}")


bytes_reader = io.BytesIO(serialized_data)
decoder = avro.io.BinaryDecoder(bytes_reader)
deserialized_user = datum_reader.read(decoder)

print(f"Deserialized data (without email): {deserialized_user}")
```

#### Output:
```plaintext
Serialized data (without email): b'\x0cJane Doe2\x00'
Deserialized data (without email): {'name': 'Jane Doe', 'age': 25, 'email': None}
```

### 3. **Writing Avro Data to a File (Python)**

In real-world applications, Avro data is often written to files, especially in big data processing. Here's an example of how to write serialized Avro data to a file.

#### Writing Avro Data to a File:
```python
import avro.datafile
import avro.io
import avro.schema
import io

schema_json = """
{
  "type": "record",
  "name": "User",
  "fields": [
    {"name": "name", "type": "string"},
    {"name": "age", "type": "int"},
    {"name": "email", "type": ["null", "string"], "default": null}
  ]
}
"""
schema = avro.schema.parse(schema_json)

users = [
    {"name": "John Doe", "age": 28, "email": "john.doe@example.com"},
    {"name": "Jane Doe", "age": 25, "email": None}
]

with open("users.avro", "wb") as avro_file:
    writer = avro.datafile.DataFileWriter(avro_file, avro.io.DatumWriter(), schema)
    for user in users:
        writer.append(user)
    writer.close()

print("Data written to users.avro")
```

#### Reading Avro Data from a File:
```python
with open("users.avro", "rb") as avro_file:
    reader = avro.datafile.DataFileReader(avro_file, avro.io.DatumReader())
    for user in reader:
        print(user)
    reader.close()
```

#### Output:
```plaintext
{'name': 'John Doe', 'age': 28, 'email': 'john.doe@example.com'}
{'name': 'Jane Doe', 'age': 25, 'email': None}
```

### 4. **Schema Evolution Example**

One of Avro’s powerful features is **schema evolution**. Let’s say you add a new field to the schema but still want to read data serialized with the old schema.

#### Original Schema (Old):
```json
{
  "type": "record",
  "name": "User",
  "fields": [
    {"name": "name", "type": "string"},
    {"name": "age", "type": "int"}
  ]
}
```

#### New Schema (Evolved):
```json
{
  "type": "record",
  "name": "User",
  "fields": [
    {"name": "name", "type": "string"},
    {"name": "age", "type": "int"},
    {"name": "email", "type": ["null", "string"], "default": null}
  ]
}
```

#### Python Example (Handling Schema Evolution):
If data was written with the old schema, you can still read it with the new schema that has the `email` field added.

```python
new_schema_json = """
{
  "type": "record",
  "name": "User",
  "fields": [
    {"name": "name", "type": "string"},
    {"name": "age", "type": "int"},
    {"name": "email", "type": ["null", "string"], "default": null}
  ]
}
"""

new_schema = avro.schema.parse(new_schema_json)

with open("users_old.avro", "rb") as avro_file:
    reader = avro.datafile.DataFileReader(avro_file, avro.io.DatumReader(new_schema))
    for user in reader:
        print(user) 
    reader.close()
```

#### Output:
```plaintext
{'name': 'John Doe', 'age': 28, 'email': None}
{'name': 'Jane Doe', 'age': 25, 'email': None}
